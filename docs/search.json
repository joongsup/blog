[
  {
    "objectID": "posts/r-markdown-revisited/index.html",
    "href": "posts/r-markdown-revisited/index.html",
    "title": "R Markdown in Vim",
    "section": "",
    "text": "I saw this tweet a couple of days ago and decided to look for ways to use R Markdown more at work.\n\n\n{{% tweet \"1108925218850893832\" %}}\n\n\nIt’s not like I haven’t tried to use R Markdown before1, but at the time of this post, I hadn’t been able to find a way to effectively incorporate R Markdown at work for various reasons. Since then, however, I added a couple of new tools to my data analysis toolbox2, which I felt might help make it work this time using R Markdown more at work.\n\n\nAnd there was a Vim plug-in for R Markdown indeed! In fact, a quick online search led me to three relevant/required Vim plug-ins: vim-rmarkdown, vim-pandoc, and vim-pandoc-syntax.\nFollowing the plug-in instructions, as well as this helpful configuration tip, I was up and running with vim-rmarkdown plug-in in no time.\n\n\n\n\n\nRendering R Markdown Document from within Vim\n\n\n\n\nIn addition to the syntax highlighting (from vim-pandoc/vim-pandoc-syntax plug-ins), the vim-rmarkdown plug-in provides a function to render a source R Markdown document into output types of interest (e.g., html) from within the source R Markdown document in Vim. :RMarkdown command does the job, and it displays successful run message once the rendering is complete3.\n\n\n\nR Markdown documents can also be rendered from R too, by the usual rmarkdown::render() function. Generally one can review the rendered output by clicking the output file, but what if such clicking/reviewing is not an option? That’s a challenge I’m facing at work. Ok, maybe there’s a way to click/review a remote file, and if that’s the case, then nobody hasn’t told me how to just yet :)\nI’m sure there’s a better way, but the workaround I came up with has two steps:\n\nRender R Markdown document (rmarkdown::render())\nSend an email to self with rendered output as attachment (sendmailR::sendmail()).\n\nI didn’t know about the sendmailR package before, but the package lets you send emails from within R. I ended up writing a couple of wrapper functions (saved in my personal package) so that rendering and sending email can be done all at once by a single function call. Yes there are some drawbacks4, but I’ve been happy with the workflow so far."
  },
  {
    "objectID": "posts/r-markdown-revisited/index.html#two-ways-to-render-r-markdown-documents",
    "href": "posts/r-markdown-revisited/index.html#two-ways-to-render-r-markdown-documents",
    "title": "R Markdown in Vim",
    "section": "",
    "text": "I saw this tweet a couple of days ago and decided to look for ways to use R Markdown more at work.\n\n\n{{% tweet \"1108925218850893832\" %}}\n\n\nIt’s not like I haven’t tried to use R Markdown before1, but at the time of this post, I hadn’t been able to find a way to effectively incorporate R Markdown at work for various reasons. Since then, however, I added a couple of new tools to my data analysis toolbox2, which I felt might help make it work this time using R Markdown more at work.\n\n\nAnd there was a Vim plug-in for R Markdown indeed! In fact, a quick online search led me to three relevant/required Vim plug-ins: vim-rmarkdown, vim-pandoc, and vim-pandoc-syntax.\nFollowing the plug-in instructions, as well as this helpful configuration tip, I was up and running with vim-rmarkdown plug-in in no time.\n\n\n\n\n\nRendering R Markdown Document from within Vim\n\n\n\n\nIn addition to the syntax highlighting (from vim-pandoc/vim-pandoc-syntax plug-ins), the vim-rmarkdown plug-in provides a function to render a source R Markdown document into output types of interest (e.g., html) from within the source R Markdown document in Vim. :RMarkdown command does the job, and it displays successful run message once the rendering is complete3.\n\n\n\nR Markdown documents can also be rendered from R too, by the usual rmarkdown::render() function. Generally one can review the rendered output by clicking the output file, but what if such clicking/reviewing is not an option? That’s a challenge I’m facing at work. Ok, maybe there’s a way to click/review a remote file, and if that’s the case, then nobody hasn’t told me how to just yet :)\nI’m sure there’s a better way, but the workaround I came up with has two steps:\n\nRender R Markdown document (rmarkdown::render())\nSend an email to self with rendered output as attachment (sendmailR::sendmail()).\n\nI didn’t know about the sendmailR package before, but the package lets you send emails from within R. I ended up writing a couple of wrapper functions (saved in my personal package) so that rendering and sending email can be done all at once by a single function call. Yes there are some drawbacks4, but I’ve been happy with the workflow so far."
  },
  {
    "objectID": "posts/r-markdown-revisited/index.html#footnotes",
    "href": "posts/r-markdown-revisited/index.html#footnotes",
    "title": "R Markdown in Vim",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAfter all, I use R Markdown for all the posts in this blog :)↩︎\nE.g., incorporating Vim-slime and GNU Make↩︎\nMaybe I’m missing something, but it would be even better if it shows the rendering stage, e.g., completion %↩︎\nOne main drawback I found, ironically, is that I have yet to find a way to download the source R Markdown document using the code_download option as in the above tweet. Maybe the source R Markdown document also needs to be sent via email along with the output?↩︎"
  },
  {
    "objectID": "posts/rjava-and-mac-os/index.html",
    "href": "posts/rjava-and-mac-os/index.html",
    "title": "rJava and Mac OS",
    "section": "",
    "text": "For the past couple of days, I needed to install rJava package on my Mac, and boy did I know how complicated the process would turn out to be! I vaguely knew about the mess that is java and Mac OS, but I didn’t care that much until now, because (1) most of my development work has been done in my company environment (Linux) which I ssh into and is pretty well managed by folks more qualified to do so than me and (2) for my personal work (i.e., blogging, R package development, etc.), I didn’t need to touch java so far (or so I think).\nThen I thought of writing a wrapper function that makes a connection from within R to a company database1. I had a snippet of code that does the job, and I decided to turn it into an R function and include it to my personal R package. Among other things, the code required rJava package, and that’s how I entered this mess that is java and Mac OS.\nSince I’m no expert there, I’m not going to try to explain each step I took, but basically, I searched for similar problems and solution, and I ended up following the awesome instruction given here. I should’ve taken screenshots of the error/ok messages that I got through this whole process, but long story short, I accidentally dropped “lib” at the end of this line where LDFLAGS2 is being set, and it took a while to figure out what I was doing wrong:\nOnce I fixed the error, rJava was installed successfully, and the database connection wrapper function is added to the personal package. Special thanks to the author of the above blog post and the author of the rJava package!"
  },
  {
    "objectID": "posts/rjava-and-mac-os/index.html#footnotes",
    "href": "posts/rjava-and-mac-os/index.html#footnotes",
    "title": "rJava and Mac OS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMaking a database connection should be doable via the Connections pane in RStudio, but let’s assume it can’t be used for whatever reason.↩︎\nDon’t even know what it is, but I’m ok with that for now.↩︎"
  },
  {
    "objectID": "posts/home-sweet-dome/index.html",
    "href": "posts/home-sweet-dome/index.html",
    "title": "Home sweet dome",
    "section": "",
    "text": "I ran into a post on the Tar Heel Blog (THB) that talks about the Tar Heel’s home court advantage in recent years. Since it’s been a while since I wrote anything about UNCMBB, I thought it’d be a great topic to write on here too, looking at how great the teams have played on home court1 in recent years. And maybe I’ll look at how Duke has played on their home court too during the same time period just because.\n`summarise()` has grouped output by 'School', 'Season'. You can override using\nthe `.groups` argument.\nTHB counted the wins and losses in the past 3 years in particular, and that gave me a starting point.\n`summarise()` has grouped output by 'Season'. You can override using the\n`.groups` argument.\n\n\n\n\n\nSeason\nWhere\nwins\nlosses\n\n\n\n\n2018\nH\n12\n3\n\n\n2019\nH\n14\n2\n\n\n2020\nH\n7\n8\nSo it seems my count and their count matches. Then I was curious if this last 3 years have been the best there is, in terms of fewest losses, and for that matter, most wins, and best winning percentage at home. In below charts, season is the starting season of the 3 years. E.g., for season = 2016, value = 4 means, rolling 3 years loss count for 2016, 2017, and 2018 seasons. Championship seasons are represented with larger dots, and the latest 3 year statistics (wins, losses, and winning percentages) with coloured horizontal lines.\nAlthough this past 3 years loss count (4) is a great feat, it’s not the best there is, it turns out, and there’s been 18 instances with fewer than 4 loss count. Three early championship seasons (1957, 1982, and 1983) stand out with 3 or fewer losses each (again, as the starting season of the next 3 seasons), with the next two championship seasons (2005 and 2009) not so superb loss counts at 6 apiece2.\nThe fewest 3-yr rolling home loss counts came in two separate instances, with a single lose apiece: 1977/1978/1979 seasons and 1978/1979/1980 seasons. Let’s see what those losses were.\nSeason\nGame_Date\nGame_Day\nType\nWhere\nOpponent_School\nResult\nTm\nOpp\nOT\n\n\n\n\n1977\n1977-01-26\nWed\nREG\nH\nWake Forest\nL\n66\n67\nNA\n\n\n1980\n1980-01-20\nSun\nREG\nH\nMaryland\nL\n86\n92\nNA\nNothing stands out at me right away about Carolina’s 1977 - 1980 teams3, let alone 1977 Wake Forest and 1980 Maryland, but it must have been fantastic home court winning streaks for the Tar Heels. Below shows the 3-yr rolling win counts.\nSure, 1977 to 1979 seasons show great home winning trends at the time (e.g., 35 home wins during 3 seasons starting from 1978 season was the most in UNC history4 at the time), but the number of games played are somewhat different year after year, so let’s look at the winning percentage instead.\nYet again, 3 years following 1977 and 1978 seasons showed the highest rolling 3-yr home winning percentages, so it must have been really fun watching Carolina playing and winning at home those days. I feel like I have some homework to do, getting to know them a little better.\nNow that I’ve looked at a couple of statistics for rolling 3 years, I became curious which freshman class has the bragging rights in terms of fewest losses, most wins, and highest winning percentage over their college carrers at home court. This should be pretty straightforward since a typical college career lasts for 4 years, which means all I have to change is the number of rolling years from 3 to 4. Below charts show the rolling 4-yr losses, wins, and win percentages, respectively.\nFor Carolina, it seems the bragging rights go to the freshman classes in the late 70s whereas for Duke, they could go to any of the teams in the late 80s, early and late 2000s. I had expected Psycho-T’s 2005 freshman class to stand out in any of the three numbers, but to my surprise the class wasn’t the top although they were close especially in terms of rolling 4-yr wins. That got me thinking, maybe they did much better in away games (after all, that class never lost to Duke on their home court during their college careers), and that’s what I’ll be looking at next time."
  },
  {
    "objectID": "posts/home-sweet-dome/index.html#footnotes",
    "href": "posts/home-sweet-dome/index.html#footnotes",
    "title": "Home sweet dome",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich, by the way, is deservingly going to be named after Coach Roy Williams.↩︎\nMost recent championship season (2017) does not have 3-yr rolling counts yet.↩︎\nJust a casual fan here :)↩︎\nSince 1949-1950 season to be exact.↩︎"
  },
  {
    "objectID": "posts/gnu-make-for-workflow-manager/index.html",
    "href": "posts/gnu-make-for-workflow-manager/index.html",
    "title": "GNU Make for Data Analysis Workflow Management",
    "section": "",
    "text": "I’ve finally started using GNU make as a data analysis workflow management tool. I knew it existed as a software “build” tool, and although I always thought Makefile1 sound pretty cool, I never actually had to use it, not just as a build tool, but also as a data analysis workflow management tool.\nIt started with @thosjleepr’s tweet that showed up in my feed:\nI never actually looked closely at what the author did in this nice tutorial, but rather ended up taking a detour and actually started looking more closely into using make as a workflow management tool. It so happened that at the time, I needed to keep running some set of scripts over and over depending on “refresh” status of each step, and so when I ran into this tweet, I decided to jump on using make finally, and now make is one of those topics that I wish I had learned and started using while at school2.\nIt’s been less than 2 weeks since I started using make, and I’m not a make expert by no means. Nonetheless, I like talking about data anlaysis workflow, so here’s my thoughts on using make as a data analysis workflow management tool as a reference. Since it’s my thoughts as of today, I’m sure some of my takes will change over time.\nFrom the Wikipedia on make (software):\nIt makes sense many tutorials on make (especially in the context of using it together with R) that I encountered used generating some documents as a use case. E.g., use make to streamline updating plots and inserting newly refreshed plots in the final document output. My immediate need didn’t involve updating a report over and over, but rather looked something like Figure @ref(fig:workflow):\nBase Workflow\nBasically this example workflow involves generating/refreshing several hive tables, training ML models, and joining hive tables. Sometimes all 5 steps need to be run in a proper sequence, other times only a subset of steps need to be run, like Figure @ref(fig:scenarios):\nWorkflow Scenarios\n\n\n\n\n\n\n\nWorkflow Scenarios\nSo if there’s anyone out there who’s on the edge and haven’t started using a tool like make for workflow management, thinking such tool is not relevant in data analysis, I hope above figures (and this post) provide some convincing argument for using it.\nA Makefile is a collection of one or more rules, with a single rule consisting of a target, dependencies, and commands. In order to use commands in a Makefile, it’s imperative that the codes can be run in command line, and not just in interactive environment (REPL).\nI don’t know what’s the best way to write a “program” for a data analysis project, and many of my codes are still mostly used in interactive mode. With the use of make, however, I’m putting more efforts to make sure my codes are run in command line, and REPL is used only for checking snippet of codes, not the entirety. So instead of working exclusively in a REPL shell (say R and/or hive shell), I now try to make sure the script files work by starting them in command line.\nThis is an important shift, I think, that has more implications, and I’m sure I’ll have more to say about this shift in the future.\nThis is a nice little benefit of using make that I liked nonetheless. When I work on a project for 2-3 days, take a break from it, and come back in 2-3 weeks, I want to be able to recall what I have done and start picking up on things with minimal effort. With a combination of comments and a sed command in a Makefile, helpful documentation/notes can be extracted from command line like below.\n~$ make help\n 1_gen_tbl_A: generate/refresh table A\n 2_train_model: train ml model using table A and generate table B\n 3_gen_tbl_C: generate/refresh table C (not dependent on steps 1 and 2!)\n 4_join_B_and_C: join tables B and C as a prep for step 5\n 5_compare_B_and_C: compare B and C by creating a performance table\nSoftware carpentry’s lesson on automation and make provides a how-to.\nThe tool has been around for nearly 40 years now. Obviously, I like it for several reasons as a workflow management tool, but at the same time, there are some limitations that can make it less attractive nowadays. I have not done a thorough research on this, but there seem to be many alternatives, each one of which has its own strength and weakness.\nThere are some great tutorials and documentations on GNU make, and here are a couple more links as a reference."
  },
  {
    "objectID": "posts/gnu-make-for-workflow-manager/index.html#footnotes",
    "href": "posts/gnu-make-for-workflow-manager/index.html#footnotes",
    "title": "GNU Make for Data Analysis Workflow Management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMake reads this Makefile (in the current directory) and works on the rules within it.↩︎\nI should prob start gathering these topics and write about them sometime in the future.↩︎"
  },
  {
    "objectID": "posts/run-system-commands-from-an-interactive-r-session/index.html",
    "href": "posts/run-system-commands-from-an-interactive-r-session/index.html",
    "title": "Run system commands or shell scripts from an interactive R session",
    "section": "",
    "text": "When you are working in an interactive R shell and need to run some system commands, you can use a base R function system1.\n# toy example\nsystem(\"ls\")\nThe system funtion can also be used to run custom shell scripts, like so,\nsystem(\"custom_script.sh arg1 arg2\")\nwhere custom_script.sh is a shell script that does some custom operations with arg1 and arg2. For example, let’s say I want to extract all the lines in a source file that contains the word “NOTE”. Below2 is one such shell script, show_notes.sh:\n#!/bin/sh\n\nif [ -z \"$1\" ]; then\n        echo \"Usage: ./show_notes.sh file_with_notes marker\"\nelse\n\nrm $1.$2 2&gt; /dev/null\n\necho \"#-----------------------------------------------------\" &gt;&gt; $1.$2\necho \"# $2 from\" $1 &gt;&gt; $1.$2\necho \"#-----------------------------------------------------\" &gt;&gt; $1.$2\n\nless $1 | grep $2 &gt;&gt; $1.$2\n\necho \"$2 are saved to file: $1.$2\"\nfi\nThe shell script extracts all the lines from the “file_with_notes” (arg1) that contains a word “marker” (arg2) and save it to a new file (with a specific file name). Then, from within an open interactive R shell, I can call the shell script like so:\nsystem(\"show_notes.sh explore.R NOTE\")\nThis will extracts all lines with “NOTE” in them from explore.R file and save it to explore.R.NOTE.\nThere are several comments to be made about this approach:\nFor the most part, once above two limitations are understood and resolved, the shell script can be useful.\nGoing one step further, if you find yourself using this particular shell script quite often in your data analysis workflow, you can include this shell script as well as the corresponding R wrapper function that calls the shell script (much like system(\"ls\") above) in an R package3.\nThe two main files (custom shell script and R wrapper function) need to be in a specific directory, respectively.\nshow_notes &lt;- function(src, marker = \"NOTE\"){\n\n  main_sh &lt;- system.file(\"sh\", \"show_notes.sh\", package = \"my_awesome_R_pkg\")\n  cmd &lt;- paste0(main_sh, \" \", src, \" \", marker)\n  print(cmd)\n  system(cmd)\n\n}\nNote that R’s base system.file() function is used here to search for the show_notes.sh file in a subdirectory /sh under the root directory of installed package, my_awesome_R_pkg.4\nOnce my_awesome_R_pkg is installed and loaded, you can call this R wrapper function from your open interactive R shell, like so (assuming explore.R file is in the current working directory of the open interactive R session):\nshow_notes(\"explore.R\")\nThis function call will extract all the lines from explore.R file that contains “NOTE” in them and save it to explore.R.NOTE in the same directory as the explore.R.\nThis approach can be useful, for example, if you want to keep track of intermediate narratives of your data analysis, which is done interactively in an R session with codes from a source file."
  },
  {
    "objectID": "posts/run-system-commands-from-an-interactive-r-session/index.html#footnotes",
    "href": "posts/run-system-commands-from-an-interactive-r-session/index.html#footnotes",
    "title": "Run system commands or shell scripts from an interactive R session",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBoth a newer version function system2 and a relatively new file system operations opackage fs seem great alternatives, but I haven’t tried them yet.↩︎\nNote that in addition to familiar R code chunks, you can also execute code chunks in other lanauges by calling the target language engine in place of r. For options, see here.↩︎\nCheck out Jim Hester’s 2018 rstudio conference presentation here for motivations and minimal package file structure.↩︎\nNote that source files in /inst/sh are copied to /sh when R package is installed.↩︎"
  },
  {
    "objectID": "posts/season-first-game/index.html",
    "href": "posts/season-first-game/index.html",
    "title": "Season’s first game",
    "section": "",
    "text": "UNC MBB team had its first official exhibition game of the 2017-18 season this past week against Barton college. It was the first official exhibition, because there was a scrimmage (unofficial exhibition?) with Memphis team about a week before UNC MBB faced Baron college.\nThe team’s first official regular season game is in less than 2 weeks against Northern Iowa, and just like they won the unofficial games (with questionable performance) so far, I’d expect the team to start the season with a W.\nBut then again, I’m just another casual fan here, and history might say otherwise. So I took a look at how UNC MBB played its first official games since 1949-50 season. Surely we must have won the first game of each season?\n\n\nWarning: `mutate_()` was deprecated in dplyr 0.7.0.\nℹ Please use `mutate()` instead.\nℹ See vignette('programming') for more help\nℹ The deprecated feature was likely used in the jutilr package.\n  Please report the issue to the authors.\n\n\n\n\n\nResult\nn\nperc\n\n\n\n\nL\n5\n0.0704225\n\n\nW\n66\n0.9295775\n\n\n\n\n\n\n\nHere we go again. Since 1949-50 season, there have been 5 seasons in which UNC MBB started a season with a L. Purely based on my memory, I knew the 2004-05 championship season started with a L against Santa Clara team, but let’s see what other seasons gave the fans rather depressing outlook for the season, albeit briefly.\n\n\n\nSeason Opener Loss\n\n\nSeason\nGame_Date\nType\nWhere\nOpponent_School\nResult\nTm\nOpp\n\n\n\n\n1966\n1965-12-01\nREG\nA\nClemson\nL\n74\n84\n\n\n1983\n1982-11-20\nREG\nN\nSt. John's\nL\n74\n78\n\n\n1997\n1996-11-22\nREG\nN\nArizona\nL\n72\n83\n\n\n2002\n2001-11-16\nREG\nH\nHampton\nL\n69\n77\n\n\n2005\n2004-11-19\nREG\nN\nSanta Clara\nL\n66\n77\n\n\n\n\n\n\n\nI’m not that familiar with 60s (and 70s) games, but it’s interesting that the 1965-66 season’s first game was against Clemson, an ACC team even back then (wasn’t sure if Clemson was in ACC back then, and found out they were). ACC confererence games don’t start until late December or early January these days, so maybe they had a different conference schedule policy back then.\n1982-83 season was a championship defending season just like upcoming 2017-18 season, and the loss against St. John’s must have given many fans an inauspicious feeling about their defending season. Let’s hope we don’t start this defending season like that, although I wouldn’t mind finishing this defending season at regional final as MJ and team did back then.\n1996-97 season is interesting, because we lost the first and last game of the season to the same team, Arizona (see table below). Not quite the same setting, but I remember feeling so bad for Michigan State team back in 2008-09 season when we beat them twice, first quite early in the season (but not the season opener), and later in the season finale in Detroit. So it must have been really depressing for the team and the fans to start and end the season that way, losing twice to the same team.\nAnd there’s that 2004-05 season opener loss to Santa Clara, but we know there’s a happy ending to this one, so I won’t go into much details for that season here. I know there is that 2001-02 season opener loss to Hampton too, but I’m pretending it didn’t even exist for obvious reason. I know it’s not fair, and that season (and failures leading up to the season) probably deserves a separate post in the future.\n\n\n\nSeason's Last Game\n\n\nSeason\nGame_Date\nType\nWhere\nOpponent_School\nResult\nTm\nOpp\n\n\n\n\n1966\n1966-03-04\nCTOURN\nN\nDuke\nL\n20\n21\n\n\n1983\n1983-03-27\nNCAA\nN\nGeorgia\nL\n77\n82\n\n\n1997\n1997-03-29\nNCAA\nN\nArizona\nL\n58\n66\n\n\n2002\n2002-03-08\nCTOURN\nN\nDuke\nL\n48\n60\n\n\n2005\n2005-04-04\nNCAA\nN\nIllinois\nW\n75\n70\n\n\n\n\n\n\n\nJust out of curiosity, I looked for seasons where the first and last game was played against the same team, and it turns out since 1949-50 season, 1996-97 season was the only such season."
  },
  {
    "objectID": "posts/winning-series/index.html",
    "href": "posts/winning-series/index.html",
    "title": "Winning Series",
    "section": "",
    "text": "Here it is, the very first trivia! Frankly I was initially curious which schools have winning series against UNC and found out which (and here’s the link to that post), but to start off things on a positive note :) I’m posting which schools have never won against UNC since 1949-50 season while having played UNC at least 10 times.\n\n\nWarning: `mutate_()` was deprecated in dplyr 0.7.0.\nℹ Please use `mutate()` instead.\nℹ See vignette('programming') for more help\nℹ The deprecated feature was likely used in the jutilr package.\n  Please report the issue to the authors.\n\n\n\n\n\n\n\n\n\nThere they are: Citadel, Rutgers, Stanford, and Tulane! Two of these teams are on UNC’s schedule for upcoming 2017-2018 season, so it’ll be interesting to see if UNC can keep the winning streak."
  },
  {
    "objectID": "posts/average-points-difference-through-first-10-games/index.html",
    "href": "posts/average-points-difference-through-first-10-games/index.html",
    "title": "Average points difference through first 10 games",
    "section": "",
    "text": "A quick update to the last post, where I looked at the (running) average of points difference through first 5 games then. 5 more games were played since then, so let’s look at the running average of points difference through the season’s first 10 games. All I have to do was to change the current number of game (from 5 to 10) and re-run the script.\n\n\n\n\n\n\n\n\n\n\n\n\nAverage points difference through season's first 10 games\n\n\nRank\nSeason\nOpponent\nTm\nOpp\nAvg_Diff\n\n\n\n\n25\n2018\nWestern Carolina\n104\n61\n17\n\n\n26\n2018\nWestern Carolina\n104\n61\n17\n\n\n\n\n\n\n\nThe impact of the first loss of the season to Spartans is evident, as the running average dropped significantly from the 5th to the 6th game, which was the lost game. With the last game win against Western Carolina, the running average gained a good momentum, and the average through the season’s first 10 games sits at 24 out of 69 seasons since 1949-50 season.\nJust for fun, let’s see how bad the loss was in terms of its impact to the running average since 1949-50 season.\n\n\n\n6th games impact to points difference running average\n\n\nSeason\nOpponent_School\nTm\nOpp\ndiffavg\ndiffavg_prev1\ndelta_p1\n\n\n\n\n1965\nIndiana\n81\n107\n8.666667\n15.6\n-6.933333\n\n\n2003\nIllinois\n65\n92\n7.166667\n14.0\n-6.833333\n\n\n1951\nEastern Kentucky\n62\n85\n9.333333\n15.8\n-6.466667\n\n\n2018\nMichigan State\n45\n63\n13.000000\n19.2\n-6.200000\n\n\n2018\nMichigan State (4)\n45\n63\n13.000000\n19.2\n-6.200000\n\n\n\n\n\n\n\nSo the loss to Spartans was one of the worst, measured by the changes to the points difference running average between 5th and 6th games, dropping the season average by more than 6 points. It’s probably not the most interesting number to quote to gauge how bad the game actually was, and many historic (and more intereting) low records have been already covered for the game.\nGood news is that the running average is increaing sharply after a trio of relatively on-par performances since the loss until the Western Carolina game, in which we played so well that many historic high records have been produced."
  },
  {
    "objectID": "posts/days-between-games/index.html",
    "href": "posts/days-between-games/index.html",
    "title": "Days between games",
    "section": "",
    "text": "After a long (exam) break, next up for the Tar Heels is Tennessee on Sunday, 12/17. 11 days since the last game against Western Carolina seemed unusually long to just another casual fan here, and I was curious how usual or unusual 11 days between any two games are.\nI started with the frequency of days between two consecutive regular season games since 1949-50 season.\n\n\n\n\n\n\n\n\n\nOk, it looks like the majority of regular season games were 3 days apart, with such games accounting for approximately 28% (or 504 games) of all the regular season games played since 1949-50 season. Next frequent pair of games were 4 days apart (21%, 389 games).\nWhat is more interesting to me, however, was the fact that there were some consecutive games played more than 2 weeks apart. What were those games anyways?\n\n\n\nConsecutive games played more than 2 weeks apart\n\n\nSeason\nnext_dt\nnext_opp\nnext_result\nlast_dt\nlast_opp\nlast_result\ndays_apart\n\n\n\n\n1950\n1949-12-28\nWest Virginia\nL\n1949-12-09\nGeorge Washington\nL\n19\n\n\n1953\n1952-12-29\nHoly Cross\nL\n1952-12-10\nClemson\nW\n19\n\n\n1954\n1954-02-02\nWashington & Lee\nW\n1954-01-19\nNorth Carolina State\nL\n14\n\n\n1955\n1955-02-04\nDuke\nL\n1955-01-18\nNorth Carolina State\nW\n17\n\n\n1956\n1956-02-04\nDuke\nL\n1956-01-18\nNorth Carolina State\nW\n17\n\n\n1957\n1957-01-30\nWestern Carolina\nW\n1957-01-15\nNorth Carolina State\nW\n15\n\n\n1958\n1958-02-01\nSouth Carolina\nW\n1958-01-18\nClemson\nW\n14\n\n\n1959\n1959-01-30\nClemson\nW\n1959-01-14\nNorth Carolina State\nW\n16\n\n\n1960\n1960-02-03\nMaryland\nW\n1960-01-16\nVirginia\nW\n18\n\n\n1962\n1962-01-06\nNotre Dame\nW\n1961-12-11\nIndiana\nL\n26\n\n\n1962\n1962-02-03\nDuke\nL\n1962-01-17\nNorth Carolina State\nW\n17\n\n\n1963\n1963-01-02\nYale\nW\n1962-12-17\nKentucky\nW\n16\n\n\n1963\n1963-02-02\nDuke\nL\n1963-01-19\nVirginia\nW\n14\n\n\n1964\n1964-01-04\nNotre Dame\nW\n1963-12-18\nGeorgia\nW\n17\n\n\n1964\n1964-02-03\nVirginia\nW\n1964-01-18\nVirginia Tech\nL\n16\n\n\n1965\n1965-01-04\nMaryland\nL\n1964-12-21\nFlorida\nL\n14\n\n\n1965\n1965-01-30\nMaryland\nL\n1965-01-16\nVirginia\nW\n14\n\n\n1966\n1966-02-03\nWake Forest\nW\n1966-01-15\nVirginia\nL\n19\n\n\n1967\n1967-01-28\nVirginia\nW\n1967-01-11\nNorth Carolina State\nW\n17\n\n\n1968\n1968-01-27\nGeorgia Tech\nW\n1968-01-13\nClemson\nW\n14\n\n\n1969\n1969-02-01\nMaryland\nW\n1969-01-18\nWake Forest\nW\n14\n\n\n1970\n1970-01-31\nMaryland\nW\n1970-01-17\nWake Forest\nL\n14\n\n\n1971\n1971-01-30\nMaryland\nW\n1971-01-16\nWake Forest\nL\n14\n\n\n1992\n1992-01-02\nPurdue\nW\n1991-12-17\nJacksonville\nW\n16\n\n\n\n\n\n\n\nSo the two games against Indiana and Notre Dame were played 26 days apart (!!!) during 1961-62 season. Wonder what was all that about. A quick search didn’t turn up anything about the specifics of those two games, but a gentle reminder that 1961-62 season was coach Smith’s first season at Carolina. I like running into things like this while exploring the match data :) (and what about those pairs of NC State/Duke games in 1954-55 and 1955-56 seasons!?)\nNext, I became curious what’s the most frequent pair of game days. Easily weekends must be the most frequent game days, I guessed.\n\n\n\n\n\n\n\n\n\nSo I was half right, because the Tar Heels indeed played the most on Saturdays, but the next most frequent game day was Wednesdays. I wonder if there’s any particular reason as to why Wednesdays have seen unusually many game actions compared to other weekdays.\nIn any case, the two observations seem to match, i.e., (1) two consecutive regular season games were played 3 days apart mostly, and (2) the games were played on Wed and Sat mostly. However, a Wed game may or may not have been followed by a Saturday game, nor a Saturday game may or may not have followed the previous Wed game. So I was wondering if indeed the two game days (Wed and Sat) are the most frequent consecutive game days.\n\n\n`summarise()` has grouped output by 'last_dy'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nSo indeed, Wed/Sat (3-day apart) was the most frequent game day pair, followed by Sat/Wed (4-day apart) pair. Below chart shows the same thing, with each row representing previous game day vs. each folumn the next game day.\n\n\n\n\n\n\n\n\n\nSo then what about the results of those games played after a long break?\n\n\n\n\n\n\n\n\n\nFor games at least 10 days apart, Tar Heels won both games right before and after breaks mostly. For example, for consecutive games that are exacxtly 10 days apart, 6 times (purple fill) we won both the last game before the break and the first game after the break, and one time (green fill) we lost the last game before the break and won the first game after the break.\nNow for Sunday’s game against Tennessee, with 11 days apart from last game, we have a chance to even the results. So far 3 times (turquoise) we won the last game before the break and lost in the first game after the break, and 2 times (purple) we won both the last game before the break and the first game after the break."
  },
  {
    "objectID": "posts/new-hugo-theme/index.html",
    "href": "posts/new-hugo-theme/index.html",
    "title": "New Hugo theme",
    "section": "",
    "text": "Last time, I wrote about a quiz from work, including the code solution. Unfortunately, I didn’t really like how the code block was printed in the minimal theme. I’m pretty sure there’s a quick and/or easy fix, but since I’m starting this blogging thing, I decided to try some other themes while at it.\nIt took some googling/reading/tweaking, but I ended up with Yihui’s XMin theme, and overall I’m satisfied with what I ended up with at the moment. It could’ve been helpful to record each and every step I took and write a blog post about it, but unfortunately, I didn’t record each and every step that I ended up taking. Nonetheless, I’ll try to summarize what I went through later for my own record, if anything."
  },
  {
    "objectID": "posts/losing-series/index.html",
    "href": "posts/losing-series/index.html",
    "title": "Losing Series",
    "section": "",
    "text": "In my first uncmbbtrivia post last week, I looked at perfect winning series of UNC MBB team, particularly looking for those schools that the team has never lost to since 1949-50 season (minimum of 10 games). Again, say hello to: Citadel, Rutgers, Stanford, and Tulane :)\nThis week, I look at the opposite end of the series history. Are there any schools that the UNC MBB team kept losing to? I know we haven’t been playing particularly well against Duke lately, so at least I knew depending on how far back I go that there will be some schools that we have losing series against.\nHow about perfect losing series? I was thinking, ok there might be some schools that have winning series aginst us, but there’s no way that there’s a school that UNC MBB team never won, not even one game, having played at least some number of games.\nSo to answer these questions, I crunched some numbers wrote some scripts (and I’ll write about this analysis process in near future), and below is what I got. First I look at schools that have winning series against UNC since 1949-50 season, having played UNC at least 10 times.\n\n\nWarning: `mutate_()` was deprecated in dplyr 0.7.0.\nℹ Please use `mutate()` instead.\nℹ See vignette('programming') for more help\nℹ The deprecated feature was likely used in the jutilr package.\n  Please report the issue to the authors.\n\n\n\n\n\n\n\n\n\nSo there they are: Indiana and Texas have winning records against UNC, having played us at least 10 times since 1949-50 season. We are not playing them this upcoming season, but as we can see in the following table, the two non-ACC teams played us quite recently, so it might take a while to be on top. Also of note is that even though we haven’t been playing well against Duke lately, we are still on top going back to 1949-50 season.\n\n\n\n\n\nOpponent_School\nfirst_game\nlast_game\ngames\nwins\nlosses\nwin_perc\n\n\n\n\nIndiana\n1961-12-11\n2016-11-30\n15\n6\n9\n0.4000\n\n\nTexas\n1992-12-05\n2018-11-22\n11\n3\n8\n0.2727\n\n\n\n\n\n\n\nThere are two variables that can give different answers to the original questions than above: starting season and minimum number of games played. As I said earlier, as much as I hate to admit, I was pretty sure we were having a losing series lately against Duke, and I was curious what other schools have been relatively better than us recently.\n\n\n\n\n\n\n\n\n\n\n\n\nOpponent_School\nfirst_game\nlast_game\ngames\nwins\nlosses\nwin_perc\n\n\n\n\nVirginia\n2000-01-18\n2020-02-15\n37\n18\n19\n0.4865\n\n\nKentucky\n2000-12-02\n2018-12-22\n18\n8\n10\n0.4444\n\n\nDuke\n2000-02-03\n2020-03-07\n49\n18\n31\n0.3673\n\n\n\n\n\n\n\nAnd there they are: Kentucky and Duke have been relatively better than us since the beginning of the millenium, although I’d think it’s only a matter of time that we’ll be on top again in the series with Kentucky. With Duke, I’m not so sure here. Numbers look pretty bad, and it might take a while to re-gain the bragging right (ok screw it. UNC &gt; Duke always Go Heels!). I’ll have to see, but I’m pretty sure the 4 seasons from the beginning of 2005 to the end of 2009 season saved us from what could have been a total embarrassment. Much more on this awesome rivalry in the future for sure.\nFinally, I was wondering if there is any school that we have never won, not even once since 1949-50 season, having played at least 2 games. It’s quite easy to do, because all I had to do was to change the variable “minimum number of games played” in the script that I used earlier. Regardless of amount of efforts involved, there shouldn’t be a school that’s having a perfect winning series against us, right?\n\n\n\n\n\n\n\n\n\n\n\n\nOpponent_School\nfirst_game\nlast_game\ngames\nwins\nlosses\nwin_perc\n\n\n\n\nNew York University\n1951-01-04\n1966-12-17\n7\n3\n4\n0.4286\n\n\nIndiana\n1961-12-11\n2016-11-30\n15\n6\n9\n0.4000\n\n\nDayton\n1967-03-24\n2010-04-01\n3\n1\n2\n0.3333\n\n\nTexas\n1992-12-05\n2018-11-22\n11\n3\n8\n0.2727\n\n\nIowa\n1989-01-07\n2014-12-03\n4\n1\n3\n0.2500\n\n\nNavy\n1950-12-28\n1998-03-12\n4\n1\n3\n0.2500\n\n\nWest Virginia\n1949-12-28\n1965-12-31\n5\n0\n5\n0.0000\n\n\n\n\n\n\n\nWell, what do I know. First of all, there are Indiana and Texas teams that I touched on in earlier part of this post (it’s sort of re-affirming to see them again in this output). Then, there go some other schools that we are having losing series records, albeit having beaten them at least once: New York University (New York University?!), Dayton, Iowa, and Navy.\nThen there is West Virgina. It turns out we have never won against West Virginia team, having played them 5 times total since 1949-50 season! Starting years were different in two cases, but it might take even longer for UNC to get even with this team than with Duke, because there’s at least two games to be played against Duke each year, whereas it seems very unlikely that we’ll see West Virginia in schedule, considering the last time we played them was on the new years eve in 1965. But what do I know. I’m just another casual fan here, trying to dig up some gems from the UNC match history data."
  },
  {
    "objectID": "posts/template-r-markdown-document-in-vim/index.html",
    "href": "posts/template-r-markdown-document-in-vim/index.html",
    "title": "R Markdown template in Vim",
    "section": "",
    "text": "In my last post, I wrote about an R Markdown centered data analysis workflow that I found to be helpful for many of my work. Helpful, in terms of improving code reusability and remembering what has been done for a particular project. Idea is that I’d start any (ad-hoc) data analysis project with a home Rmd document to store both codes and narratives.\nIt followed naturally that I soon started hoping to start a new Rmd document with some predefined default setting in Vim, e.g., metadata such as title, date, and output options, etc., as well as R packages that I’d use all the time. I knew RStudio IDE provides template functionality, but I wanted to be able to do it in the tool/environment that I spend most time in daily. So I started looking around, and quickly ran into several blog posts on how to use Vim templates1.\nIt comes down to the following two components:\nFor example, an Rmd skeleton file (e.g., skeleton.Rmd) can contain yaml headers and an R setup chunk like below.\nWhile not the main topic, I had to look up how to display my yaml headers and R code chunk verbatim in a post like above, and found out some tricks from the code repo for the R Markdown: The Definitive Guide book.\nOnce skeleton.Rmd is ready (e.g., ~/.vim/templates/skeleton.Rmd), Vim can insert the contents of the skeleton file to the newly created .Rmd file with autocmd like so:\n```{{sh, eval = FALSE}} if has(“autocmd”) augroup templates autocmd BufNewFile *.Rmd 0r ~/.vim/templates/skeleton.Rmd augroup END endif\n```\nAbove Vim codes resides in .vimrc file and reads in the skeleton.Rmd file whenever a new .Rmd file is created."
  },
  {
    "objectID": "posts/template-r-markdown-document-in-vim/index.html#footnotes",
    "href": "posts/template-r-markdown-document-in-vim/index.html#footnotes",
    "title": "R Markdown template in Vim",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI followed this blog post on how to use Vim templates↩︎"
  },
  {
    "objectID": "posts/worst-performance-at-home-sweet-dome/index.html",
    "href": "posts/worst-performance-at-home-sweet-dome/index.html",
    "title": "Coping with worst loss at home",
    "section": "",
    "text": "It’s been a tough weekend, not least because Tar Heels lost at home. Sometimes I feel like I’m vested too much in the outcome of the Heel’s basketball games, and if my emotional rollercoaster the rest of the weekend gives any clue, I probably am, just a litttle bit. I tried to shake it off, yet it wasn’t particularly easy, not only because we lost at home, but because the loss was the worst one at home under Roy Williams. So I turn to blogging, after 100+ days of hiatus, as a last resort to restoring my inner peace and calm, before the new workday begins.\nFirst I look at a couple of worst of the worst losses at home (to make the last loss less painful).\n\n\n\n10 worst home loss since 1949-50 season\n\n\nSeason\nGame_Date\nOpponent_School\nTm\nOpp\nlost_by\n\n\n\n\n1964\n1964-02-29\nDuke\n69\n104\n35\n\n\n2002\n2002-01-31\nDuke\n58\n87\n29\n\n\n2020\n2019-12-04\nOhio State\n49\n74\n25\n\n\n1951\n1950-12-20\nEastern Kentucky\n62\n85\n23\n\n\n1955\n1955-02-04\nDuke\n68\n91\n23\n\n\n1975\n1975-02-15\nMaryland\n74\n96\n22\n\n\n2002\n2002-01-05\nWake Forest\n62\n84\n22\n\n\n1953\n1953-02-21\nNorth Carolina State\n66\n87\n21\n\n\n2019\n2019-01-12\nLouisville\n62\n83\n21\n\n\n1999\n1999-02-27\nDuke\n61\n81\n20\n\n\n\n\n\n\n\nSure enough, 4 losses to Duke (gee, a loss by 35!) and a couple to State (a loss by 21, which matches the weekend’s loss margin). At this point, I’m realizing this is not particularly going to help me feel better about the weekend’s loss. Worse, it might exacerbate the wound, but I decide to keep going. What’s all the home losses look like?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTurns out, of the 122 total home losses (since 1949-50 season), loss by 4 is the most frequent (14 games), followed by loss by 1 (13 games). Sure, Duke won the most with 25 wins against UNC, followed by State (16), Wake Forest (15), and Maryland (13). Other than these 4 schools, no schools have won at UNC more than 6 times since 1949-50 season. I’m already starting to feel a little better. While at it, I decide to look at how Duke’s home loss look like for the heck of it.\n\n\n\n10 worst home loss since 1949-50 season\n\n\nSeason\nGame_Date\nOpponent_School\nTm\nOpp\nlost_by\n\n\n\n\n1959\n1958-12-29\nMichigan State\n57\n82\n25\n\n\n1960\n1960-02-27\nNorth Carolina\n50\n75\n25\n\n\n1973\n1973-02-21\nNorth Carolina State\n50\n74\n24\n\n\n1975\n1975-01-27\nNorth Carolina State\n71\n95\n24\n\n\n1975\n1975-02-08\nMaryland\n80\n104\n24\n\n\n1983\n1983-03-05\nNorth Carolina\n81\n105\n24\n\n\n1989\n1989-01-18\nNorth Carolina\n71\n91\n20\n\n\n1960\n1960-02-20\nWake Forest\n64\n83\n19\n\n\n1974\n1974-01-23\nWake Forest\n71\n90\n19\n\n\n1950\n1950-02-24\nNorth Carolina\n46\n64\n18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTurns out, of the 135 Duke’s home losses (since 1949-50 season), Carolina beat them 29 times, followed by State (21), Wake Forest (15), Maryland (14), and Virginia (9). Other than these 5 schools, no schools have beat Duke more than 6 times since 1949-50 season.\nAnother way to look at how sweet home games have been since Roy Williams came home might be to look at how rare the home losses have been.\n\n\n\nHome loss since 2003-04 season\n\n\nSeason\nGame_Date\nOpponent_School\nTm\nOpp\nlost_by\n\n\n\n\n2020\n2019-12-04\nOhio State\n49\n74\n25\n\n\n2019\n2019-01-12\nLouisville\n62\n83\n21\n\n\n2013\n2013-03-09\nDuke\n53\n69\n16\n\n\n2010\n2010-01-31\nVirginia\n60\n75\n15\n\n\n2010\n2010-01-20\nWake Forest\n69\n82\n13\n\n\n2020\n2020-01-04\nGeorgia Tech\n83\n96\n13\n\n\n2015\n2015-02-24\nNorth Carolina State\n46\n58\n12\n\n\n2006\n2006-01-14\nMiami\n70\n81\n11\n\n\n2008\n2008-02-06\nDuke\n78\n89\n11\n\n\n2015\n2015-02-02\nVirginia\n64\n75\n11\n\n\n2010\n2010-02-10\nDuke\n54\n64\n10\n\n\n2010\n2010-02-24\nFlorida State\n67\n77\n10\n\n\n2013\n2013-01-10\nMiami\n59\n68\n9\n\n\n2019\n2019-02-11\nVirginia\n61\n69\n8\n\n\n2020\n2020-01-08\nPittsburgh\n65\n73\n8\n\n\n2006\n2006-01-25\nBoston College\n74\n81\n7\n\n\n2009\n2009-01-04\nBoston College\n78\n85\n7\n\n\n2015\n2015-03-07\nDuke\n77\n84\n7\n\n\n2014\n2014-01-08\nMiami\n57\n63\n6\n\n\n2004\n2003-12-20\nWake Forest\n114\n119\n5\n\n\n2015\n2014-12-03\nIowa\n55\n60\n5\n\n\n2006\n2005-11-29\nIllinois\n64\n68\n4\n\n\n2006\n2006-02-07\nDuke\n83\n87\n4\n\n\n2018\n2017-12-20\nWofford\n75\n79\n4\n\n\n2018\n2018-01-27\nNorth Carolina State\n91\n95\n4\n\n\n2020\n2019-12-15\nWofford\n64\n68\n4\n\n\n2014\n2013-11-17\nBelmont\n80\n83\n3\n\n\n2014\n2013-12-18\nTexas\n83\n86\n3\n\n\n2018\n2018-02-27\nMiami\n88\n91\n3\n\n\n2020\n2020-01-11\nClemson\n76\n79\n3\n\n\n2004\n2004-02-05\nDuke\n81\n83\n2\n\n\n2008\n2008-01-19\nMaryland\n80\n82\n2\n\n\n2010\n2010-01-16\nGeorgia Tech\n71\n73\n2\n\n\n2020\n2020-02-08\nDuke\n96\n98\n2\n\n\n2020\n2020-02-15\nVirginia\n62\n64\n2\n\n\n2007\n2007-02-13\nVirginia Tech\n80\n81\n1\n\n\n2012\n2012-02-08\nDuke\n84\n85\n1\n\n\n2015\n2015-01-05\nNotre Dame\n70\n71\n1\n\n\n2016\n2016-02-17\nDuke\n73\n74\n1\n\n\n2020\n2020-02-01\nBoston College\n70\n71\n1\n\n\n\n\n\n\n\nOk, a good start. There’s been only 30 home losses since 2003-04 season, approximately 2 home losses per season. Only 2 down days over a 4 months period each season. Put it that way, I feel like I can survive this weekend’s L.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince 2003-04 season, again loss by 1 and 4 were most frequent, each with 4 games. Not surprisingly, Duke has won 8 games at UNC, and somewhat surprisingly, Miami has second most wins at UNC since RW came home. This weekend’s win gives Louisville its first win at Carolina, it looks like.\nIn addition to the worst home loss in 16 years, what made things worse for me personally was the fact that Duke won their game against Florida State later same day. Again, this is just me, but after the home loss, I was begging for Florida State to win their home game, so that I can have somewhat so-so weekend even with the home loss, even though I knew how rare such event was.\n\n\n\nCarolina L and Duke W on same day since 1949-50 season\n\n\nSeason\nGame_Date\nOpponent_School\nTm\nOpp\nlost_by\n\n\n\n\n2020\n2020-02-15\nVirginia\n62\n64\n2\n\n\n2020\n2020-02-01\nBoston College\n70\n71\n1\n\n\n2020\n2020-01-11\nClemson\n76\n79\n3\n\n\n2020\n2020-01-08\nPittsburgh\n65\n73\n8\n\n\n2020\n2020-01-04\nGeorgia Tech\n83\n96\n13\n\n\n2019\n2019-01-12\nLouisville\n62\n83\n21\n\n\n2018\n2017-12-20\nWofford\n75\n79\n4\n\n\n2015\n2014-12-03\nIowa\n55\n60\n5\n\n\n2009\n2009-01-04\nBoston College\n78\n85\n7\n\n\n2008\n2008-01-19\nMaryland\n80\n82\n2\n\n\n2006\n2006-01-14\nMiami\n70\n81\n11\n\n\n2004\n2003-12-20\nWake Forest\n114\n119\n5\n\n\n2003\n2002-12-07\nKentucky\n81\n98\n17\n\n\n2002\n2001-11-20\nDavidson\n54\n58\n4\n\n\n2001\n2000-12-02\nKentucky\n76\n93\n17\n\n\n2000\n2000-01-22\nFlorida State\n71\n76\n5\n\n\n1999\n1999-01-13\nMaryland\n76\n89\n13\n\n\n1989\n1989-01-07\nIowa\n97\n98\n1\n\n\n1970\n1970-02-21\nSouth Carolina\n62\n79\n17\n\n\n1968\n1968-02-28\nSouth Carolina\n86\n87\n1\n\n\n1966\n1966-02-12\nVirginia Tech\n75\n81\n6\n\n\n1966\n1965-12-31\nWest Virginia\n97\n102\n5\n\n\n1963\n1963-02-09\nWake Forest\n71\n72\n1\n\n\n1962\n1962-02-10\nWake Forest\n80\n87\n7\n\n\n1962\n1961-12-11\nIndiana\n70\n76\n6\n\n\n1953\n1953-02-21\nNorth Carolina State\n66\n87\n21\n\n\n1952\n1952-01-26\nNorth Carolina State\n53\n58\n5\n\n\n1950\n1950-02-07\nWake Forest\n54\n57\n3\n\n\n\n\n\n\n\nOk, 22 such days when Carolina lost at home while Duke won on the same day since 1949-50 season. And only 6 such days since 2003-04 season. Make it 7 with this weekend’s case, but nevertheless at the end of the day, an L is an L, and we had two road Ws before this L, and I hope the team shook it off well over the weekend. Writing this post definitely helped me feel a whole lot better about this weekend and get ready for a new day tomorrow!"
  },
  {
    "objectID": "posts/season-win-percentage/index.html",
    "href": "posts/season-win-percentage/index.html",
    "title": "Season win percentage",
    "section": "",
    "text": "This past week, I was in a presentation training, and all the participants had to select a topic on the last day of training to give a (perfect) 15-min presentation. You know, after learning all the essential presentation skills, it was time to apply those fancy skills to work!\nI decided to talk about UNC MBB and its great stories behind each of the 3 championship seasons during Williams-era. I made a 5-page deck using Rmd-driven slidy presentation, and I am planning to post it here either in its raw form or after some adjustment using xaringan.\nThe main driver of the presentation was each season’s win percentage, including all match types (regular, conference tournament, and NCAA tournament) as so:\n\n\n\n\n\n\n\n\n\nAfter I gave the final presentation, I realized that looking at regular season win % leading up to NCAA tournament might be also interesting, especially to say anything about each team’s performance during the regular season and its final result at NCAA tournament level. So I made a small change in my script, and below is the regular season win % since 1949-50 season.\n\n\n\n\n\n\n\n\n\nThere goes the perfect championship winning season in 1956-57, Dean Smith’s first championship season in 1981-82, and his second one in 1992-93 season. And there goes Roy William’s first championship season in 2004-05, second one in 2008-09 season, and last but not least, his latest in 2016-17 season. (And yes I do see that horrible deep in early 2000s.)\nThere are also some seasons that had as much success as the crowning seasons in terms of regular season win %s but without bearing the NCAA championship trophy. Looking at the top 10 seasons by the regular season win % should make it easy to identify them:\n\n\n\n\n\nSeason\ngames\nwins\nlosses\npct\nchamp\n\n\n\n\n1957\n24\n24\n0\n1.0000000\nYes\n\n\n1984\n27\n26\n1\n0.9629630\nNo\n\n\n2008\n31\n29\n2\n0.9354839\nNo\n\n\n1987\n29\n27\n2\n0.9310345\nNo\n\n\n1976\n26\n24\n2\n0.9230769\nNo\n\n\n1982\n26\n24\n2\n0.9230769\nYes\n\n\n1998\n30\n27\n3\n0.9000000\nNo\n\n\n2009\n30\n27\n3\n0.9000000\nYes\n\n\n1993\n29\n26\n3\n0.8965517\nYes\n\n\n2005\n29\n26\n3\n0.8965517\nYes\n\n\n\n\n\n\n\nInterestingly, only (?) half of the top 10 seasons with highest win % ended up with NCAA championship. Other half not-so-fortunate seasons include some that I’m familiar with (such as 2008, 1998) and others that I’m not (such as 1984, 1987, and 1976). Having players data readily available for each season could’ve made it a breeze to look into those highly successful yet not so fortunate seasons, and it’s definitely something that I’d like to add to my data source.\nAlso notable is that not all the 6 NCAA championship winning seasons are included in the list of top 10 regular season win %. Turns out the latest crowning season is not part of this elite seasons, having lost 6 games in the regular season, a feat that is not indeed so elite compared to the 5 highly successful yet unforunate seasons from above.\n\n\n\n\n\nSeason\ngames\nwins\nlosses\npct\nchamp\n\n\n\n\n1957\n24\n24\n0\n1.0000000\nYes\n\n\n1982\n26\n24\n2\n0.9230769\nYes\n\n\n2009\n30\n27\n3\n0.9000000\nYes\n\n\n1993\n29\n26\n3\n0.8965517\nYes\n\n\n2005\n29\n26\n3\n0.8965517\nYes\n\n\n2017\n32\n26\n6\n0.8125000\nYes\n\n\n\n\n\n\n\nBut between a highly successful regular season without trophy versus a lukewarm (I don’t know, is 26-6 a lukewarm season?) regular season with a trophy at the end? I’ll take the trophy day in and day out!"
  },
  {
    "objectID": "posts/insert-images-in-blogdown-post/index.html",
    "href": "posts/insert-images-in-blogdown-post/index.html",
    "title": "Insert images in blogdown post, static directory, and xaringan",
    "section": "",
    "text": "While working on yet another separate blog post, I needed to insert images in the post. I knew from rmarkdown syntax that I can use: ![image name](path to file), but then I didn’t know where the image files need to be.\nIt turns out I can have the image files under the static/ directory, everything under which will be copied to public directory.\nMoreover, static/ directory can also be used to build Rmd documents (e.g., pdf and HTML5), and I decided to try generating an HTML5 slide using xaringan package. It took some trial-and-error but eventually I got it working and here’s the slide!\n(Next, maybe I should consider breaking out posts and slides.)"
  },
  {
    "objectID": "posts/rmarkdown-centered-data-analysis-workflow/index.html",
    "href": "posts/rmarkdown-centered-data-analysis-workflow/index.html",
    "title": "R Markdown centered data analysis workflow",
    "section": "",
    "text": "A summary is here, and the codes are here.\nI’m a workflow fanatic. Ok, fanatic maybe too strong a word, but that’s what I told one of my R stars when I met her in person recently, who’s also passionate about (in my views) project workflow and “generally doing basic stuff really, really well”. I can’t pinpoint how and why I became so interested in establishing a good project workflow, but it’s one of those things that I really wish I had mastered while I was still in school. Maybe the lack of such training during graduate school led me down this path of caring about research computing education as well as getting involved with Software/Data Carpentry, which I wish I had known about it back then1."
  },
  {
    "objectID": "posts/rmarkdown-centered-data-analysis-workflow/index.html#tldr",
    "href": "posts/rmarkdown-centered-data-analysis-workflow/index.html#tldr",
    "title": "R Markdown centered data analysis workflow",
    "section": "",
    "text": "A summary is here, and the codes are here.\nI’m a workflow fanatic. Ok, fanatic maybe too strong a word, but that’s what I told one of my R stars when I met her in person recently, who’s also passionate about (in my views) project workflow and “generally doing basic stuff really, really well”. I can’t pinpoint how and why I became so interested in establishing a good project workflow, but it’s one of those things that I really wish I had mastered while I was still in school. Maybe the lack of such training during graduate school led me down this path of caring about research computing education as well as getting involved with Software/Data Carpentry, which I wish I had known about it back then1."
  },
  {
    "objectID": "posts/rmarkdown-centered-data-analysis-workflow/index.html#my-pursuit-of-a-data-analysis-workflow",
    "href": "posts/rmarkdown-centered-data-analysis-workflow/index.html#my-pursuit-of-a-data-analysis-workflow",
    "title": "R Markdown centered data analysis workflow",
    "section": "My pursuit of a data analysis workflow",
    "text": "My pursuit of a data analysis workflow\n(also something about notebooks)\nIn any case, coming up with a workflow that suits me for most of my needs either at home or at work has been a long journey of mine, and I’ve written a couple of posts on here for some “milestones”.\n\nR Markdown in Vim\nVim, vim-slime, and screen\nGNU Make for Data Analysis Workflow Management\nComment on data analysis workflow\nRun system commands or shell scripts from an interactive R session\n\nI haven’t written about it yet, but I also started using Python pretty heavily at work, and my interest in workflow since then has been about how to put together codes (R/Python/SQL) and by-products (findings/plots) more efficiently in R Markdown. The idea of combining codes and prose is not new at all, with ever growing popularity of using notebooks by data scientists for doing just that. I have used notebooks too (e.g., RCloud, Jupyter Notebook, etc.) in the past, but for various reasons it never stuck with me. It’s still hard to pinpoint exatly what, but if I have to guess, among many pros and cons of notebooks, what didn’t work for me was the lack of the final/intermediate by-product (e.g., a summary document with findings, tables, and plots) that I can refer back to anytime in the future. Most likely due to how I used it (inefficiently, that is), my notebooks were always in-progress, never-ending, ever-changing, without the merit of reusable codes. Yihui’s post on notebooks, IDEs, and R Markdown comes to mind."
  },
  {
    "objectID": "posts/rmarkdown-centered-data-analysis-workflow/index.html#what-im-hoping-for-in-a-data-analysis-workflow",
    "href": "posts/rmarkdown-centered-data-analysis-workflow/index.html#what-im-hoping-for-in-a-data-analysis-workflow",
    "title": "R Markdown centered data analysis workflow",
    "section": "What I’m hoping for in a data analysis workflow",
    "text": "What I’m hoping for in a data analysis workflow\nR Markdown seemed a promising component of a workflow, especially in terms of being a central place where I can document what I tried, which scripts I used, and finally the findings from each code exeucution that can help me understand a project’s “whereabout” anytime I needed to. And here I’m hoping my workflow to\n\ninclude Vim, my text editor of choice\nbe language-agnostic (within R and Python)\nallow quick interactive code checking\nproduce a summary document that includes codes/tables/plots/findings\npromote code reusability\n\nAfter several trial-and-error, I feel like what I have at the moment is a good working version of an R Markdown centered workflow, specifically for data analysis using text editor of choice, Vim. I understand there are several components here that can make this workflow rather specific than general (e.g., Vim, not RStudio), but I believe the general idea can still be helpful for others as well."
  },
  {
    "objectID": "posts/rmarkdown-centered-data-analysis-workflow/index.html#so-whats-it-look-like",
    "href": "posts/rmarkdown-centered-data-analysis-workflow/index.html#so-whats-it-look-like",
    "title": "R Markdown centered data analysis workflow",
    "section": "So, what’s it look like?",
    "text": "So, what’s it look like?\nInitially, I was going to write about the workflow with an example in this blog post, but since it included a working example, it became a bit confusing structure-wise to include the working example (I’m sure there’s a way to do it), so I decided to use github repo instead for code examples as well as the final summary document. So think of this blog post as an introduction/motivation for the actual work that is stored in github.\nAs the subtitle of the main document in the repo says, I also talk about how to use Python codes in R Markdown using reticulate package. In fact, three types of coding examples are given:\n\nhow to use R codes\nhow to use Python codes\nhow to use R objects in Python, and Python objects in R\n\nFor each type of examples, three different scenarios are used in terms of how to incorporate corresponding codes into the R Markdown document.\n\nwork with code snippets directly in R Markdown\nimport/source function definitions from separate script files and use them in R Markdown\ndisplay outputs from separate script files in R Markdown\n\nAlthough not perfect and still under development, I feel like I now have a portable and transferable workflow for data analysis that I can mix and match depending on the scope of any projects. As someone said, just trying to do “basic stuff really, really well”, so that it just “flows”!"
  },
  {
    "objectID": "posts/rmarkdown-centered-data-analysis-workflow/index.html#relevantfuture-work",
    "href": "posts/rmarkdown-centered-data-analysis-workflow/index.html#relevantfuture-work",
    "title": "R Markdown centered data analysis workflow",
    "section": "Relevant/future work",
    "text": "Relevant/future work\n\nAround the time I started reconsidering R Markdown as part of my workflow, I ran into Emily Riederer’s post on RMarkdown driven development (nicely worded!), and it’s a great read. I haven’t tried to link my workflow in terms of packaging, which I also care about a lot, but such conversion seems an interesting/challenging idea. E.g., as it stands, my home R Markdown document lives in the root project directory, which is not allowed in R package structure. I’m sure I can just move the R Markdown to /doc directory, but there could be something I’m missing here.\nJust last night, I ran into Miles McBain’s tweet that was something about Rmd and reproducibility (so timely!):\n\nInteractive use (of R and Python shells) in a data analysis project workflow and “search path” are two topics that I think about quite a lot these days, but in terms of loss of reproducibility when Rmd files are rendered in console, I must have been impacted by it, as in the workflow I described in this post, I render my home R Markdown document in console. Either I might have overlooked its impact or I haven’t run into any such loss of reproducibility."
  },
  {
    "objectID": "posts/rmarkdown-centered-data-analysis-workflow/index.html#footnotes",
    "href": "posts/rmarkdown-centered-data-analysis-workflow/index.html#footnotes",
    "title": "R Markdown centered data analysis workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt seems they were getting seriuos just as I was wrapping up my time in school↩︎"
  },
  {
    "objectID": "posts/quiz-from-work/index.html",
    "href": "posts/quiz-from-work/index.html",
    "title": "Quiz from work",
    "section": "",
    "text": "There are 25 black balls and 25 white balls in a jar. We take two balls at random from the jar, and the one of the three: (1) if two black balls are drawn, put them back in to the jar; (2) if two white balls are drawn, throw them away; (3) if mixed, put a white ball back in, and throw away the black ball. We win this game when a white ball is the only ball remaining in the jar. Can we win the game, and if yes, what’s the probability that we win the game?\n\nSince this quiz was asked in a “coding challenge” session at work, I went ahead and wrote the following to see what happens, even though I suspected this could be solved with pen/paper, especially because the quiz was introduced as a question that had been asked at a job interview.\n\ndraw_balls &lt;- function(balls){ # balls = list(begin_w, begin_b)\n  w &lt;- balls[[1]]\n  b &lt;- balls[[2]]\n  #print(paste0(\"Beginning balls (w, b): \", w, \", \", b))\n  \n  # note I'm assuming two balls are drawn one at a time, not together\n  # other drawing rules are possible\n  # drawing rule for first ball\n  if (runif(1) &lt; w/(w + b)) {\n    wtmp1 &lt;- 1 # draw white\n    btmp1 &lt;- 0\n  } else {\n    wtmp1 &lt;- 0\n    btmp1 &lt;- 1 # draw black\n  }\n      \n  # drawing rule for second ball\n  if (runif(1) &lt; (w - wtmp1)/(w - wtmp1 + b - btmp1)){\n    wtmp2 &lt;- 1 # draw white\n    btmp2 &lt;- 0\n  } else {\n    wtmp2 &lt;- 0\n    btmp2 &lt;- 1 # draw black\n  } \n      \n  # post-draw actions\n \n  if (wtmp1 == 1 & wtmp2 == 1) {\n    #print(\"Balls drawn: 2 W\")\n    w &lt;- w - (wtmp1 + wtmp2)\n    b &lt;- b\n  } else if (btmp1 == 1 & btmp2 == 1){\n    #print(\"Balls drawn: 2 B\")\n    w &lt;- w\n    b &lt;- b\n  } else { # if mixed\n    #print(\"Balls drawn: mixed\")\n    w &lt;- w\n    b &lt;- b - 1\n  }\n\n  #print(paste0(\"Ending balls (w, b): \", w, \", \", b))\n  balls &lt;- list(ending_w = w, ending_b = b)\n}\n\n# stopping rule1: if whilte ball = 0, black balls &gt;= 1; then white loses\n# stopping rule2: if black ball = 0, white ball odd numbers are left; then white wins\n# stopping rule3: if black ball = 0, white ball even numbers are left; then white loses\n\niterate_draw_balls &lt;- function(count = 0, begin_w, begin_b){\n\n  while(begin_w * begin_b &gt; 0){\n    #print(\"==============================\")\n    count &lt;- count + 1\n    #print(paste0(\"Count: \", count))\n    \n    a &lt;- draw_balls(list(begin_w, begin_b))\n    begin_w &lt;- a$ending_w\n    begin_b &lt;- a$ending_b\n  }\n\n  \n  if (begin_w == 0){ # stopping rule 1\n    win &lt;- 0\n    #print(\"The last ball was a black one\")\n    win\n  } else if (begin_w %% 2 == 1) { # stopping rule 2\n    win &lt;- 1\n    #print(\"The last ball was a white one.\")\n    win\n  } else if (begin_w %% 2 == 0) { # stopping rule 3\n    win &lt;- 0\n    #print(\"There is no ball left.\")\n    win\n  } else {\n    #print(\"Shouldn't see this!\")\n    win &lt;- -9999999999\n  }\n}\n\n#--------------------------------\n# run the monte carlo simulation\n#--------------------------------\n\nset.seed(1234)\nN &lt;- 100\nbegin_w &lt;- 25\nbegin_b &lt;- 25\nresults &lt;- vector(\"list\", length = N)\nfor (sim in 1:N){\n  #print(paste0(\"Simulation iteration: \", sim))\n  results[[sim]] &lt;- iterate_draw_balls(count = 0, begin_w = begin_w, begin_b = begin_b)\n  #print(paste0(\"Simulation result: \", results[[sim]]))\n}\nsum(unlist(results))/N\n\n[1] 1\n\n\nFor different values of N, the results were the same, depending on whether the number of white balls was odd or even. So the immediate answer to the quiz seems to be that we always win, since we started the game with 25 white balls.\nSeeing such pattern, I thought about state space of white and black balls in terms of them being odd or even, and after tinkering with different transition cases, it confirmed my simulation experiment."
  },
  {
    "objectID": "posts/rss-feed/index.html",
    "href": "posts/rss-feed/index.html",
    "title": "RSS feed and rbind github repo",
    "section": "",
    "text": "Maëlle Salmon reminded me to submit posts/blog RSS feed to R weekly on my tweet, and I set out to do just that. I didn’t have much understanding as to how to though, and googling things like “how to generate rss feed in xmin blogdown” didn’t really turn up anything useful for new bloggers like me. Maybe it’s a quite straighforward knowledge that my full blog url is not a valid RSS feed address, but I got to know that only after checking my blog url from the R weekly submit page. And besides, I’m pretty sure for most blogs that I wanted to add to my Feedly, entering their url address worked just fine, and it was a bit of a head scratcher.\nA bit of digression before getting into how I was able to turn on RSS feed. Since my main excitement with blogdown was being able to focus on content generation (mostly about data science and UNC basketball) and not having to worry about the site set-up, deployment, etc., I naturally followed Yihui’s recommneded workflow of using netlify for the site deployment. I was very pleased with how easy it was to start my own blog and kept going for quite some time. Then I quickly ran into a little identity crisis (as my netlify subdomain was “uncmbbtrivia”1), and I decided to get a subdomain under rbind.io, which I had known about before. With the domain change, I’d change my subdomain name too, I figured.\nSo I set out to switch over to rbind.io and ran into rbind support’s about page. I opened a ticket, and Nan Xiao quickly helped me with .rbind.io subdomain creation, and now my blog’s at rbind just like that2. Without a doubt, it was a very pleasant experience, and I was all set.\nExcept that my source files were still residing in my person github account, when I had expected that the source repo would have somehow automatically transferred to rbind github account when my rbind.io subdomain was created. At least that’s how I read the how to join rbind section in the about page, but it wasn’t the case, and I ended up transferring my blog source repo to rbind organization repo on github, literally with a click of a button3. So I joined the other 150+ people (or 60+ repos) that are hosted under rbind github account.\nBack to the RSS feed story, so I needed some quick help on how to generate an RSS feed, and I could’ve just fired up a question on twitterverse and/or opened a ticket on rbind github, but I decided to make use of the rbind repo. These are the people who more or less have the same interest as me, especially in using R for data science and also likely blogdown for blogging, so I should be able to find some clues as to how to generate an RSS feed, I figured.\nAnd I did find what I was missing there! I noticed that I didn’t have /layouts/rss.xml file4. Once I manually added the file and inserted a couple of lines in my /layouts/partials/header.html, I was all set, and my RSS feed was generated successfully for now5.\nSo there, I was happy to have my blog subdomain in .rbind.io6, and I’m also happy to have my source repo under rbind github account, which can “make it easier to discover websites and get inspirations from other people’s websites”, and “…may study its source files, or even ask the author via Github issues.”"
  },
  {
    "objectID": "posts/rss-feed/index.html#footnotes",
    "href": "posts/rss-feed/index.html#footnotes",
    "title": "RSS feed and rbind github repo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI understand I could’ve changed the netlify subdomain, but opted to rbind anyway.↩︎\nThe netlify address still works, but it forwards to rbind. Notes on forwarding here, thanks to @yihui.↩︎\nAnd I think I need to change my local git setting too, at least that’s the message I get whenever I commit/push changes, although it seems like it’s not critical at the moment.↩︎\nMaybe it was not needed, and I could’ve turned on RSS feed somehow differently?↩︎\nWhen I validated the RSS link, it gave me some recommendations, and I haven’t spent time to address them yet, but they don’t seem to be errors.↩︎\nBesides the new name, my other attraction to rbind.io is that rbind.io address automatically tells the world that the blogger cares for R.↩︎"
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello World!",
    "section": "",
    "text": "Welcome to UNC MBB TRIVIA! Just another casual fan here to talk about UNC Men’s Basketball trivia time to time for all Tar Heels fans to enjoy. Stay tuned!"
  },
  {
    "objectID": "posts/vim-mode-in-rstudio/index.html",
    "href": "posts/vim-mode-in-rstudio/index.html",
    "title": "Vim mode in RStudio",
    "section": "",
    "text": "I heard about Vim mode in RStudio but haven’t really given it a try. While working on a new blog post (not this one), I decided to give it a try, as I had to keep switching between non-Vim mode on my local Mac and Vim mode in my remote server (Linux). I’m not a Vim expert by any measure, but for some reason, I like working in Vim.\nSo the option is in Tools/Global Options/Code/Key Bindings, and boom, I started using Vim in RStudio. One thing I ran into immediately, though, was that the cursor movement keys (h/j/k/l) wouldn’t repeat themselves when I kept them pressed. At first I thought maybe it’s one of those things that I’d have to live with, but quickly it became rather uncomfortable having to repeatedly pressing j/k keys to go up and down.\nTurns out all I had to do was to change system reference (on my Mac) as explained in this support page.\nI feel like there must be some more (unexpected) additional features that come with Vim mode in RStudio (not sure if it’s intended or not), such as file save by cmd-s, which I find really helpful, because I don’t have to do esc-:-w just to save file :)"
  },
  {
    "objectID": "posts/comments-on-data-analysis-workflow/index.html",
    "href": "posts/comments-on-data-analysis-workflow/index.html",
    "title": "Comments on data analysis workflow",
    "section": "",
    "text": "There are several benefits of establishing a good and routine data analysis workflow that you follow on a daily basis. At least two benefits come to mind immediately.\n\nHaving a good data analysis workflow is beneficial and needed to do reproducible research/work (RR). RR could mean one thing to one and quite another to others, but to me, doing reproducible work means specifically doing my work in a fashion that allows me to pick up from last touched point in that particular work after some break. For example, after spending 2-3 days on project A, I might go off to do something else for the next 2-3 days, and when I come back to project A, I want to know exactly what’s been done and what I need to do. There are many components of data analysis workflow that can be helpful for one to do RR, and I try to outline them below.\nA routine workflow helps us avoid mental fatigue. This may be from one of the books by Hadley, but to me, this means knowing what, where, why, and how to do things right off the bat when starting a new project. Slowly but surely, I’m settling down to a specific approach that I’ll touch upon later, but to get to this point, I tried several different alternatives, and many times ended up failing, for example, to keep track of how to do things (e.g., refresh data with new source data) all over again when needed. Typical challenges have been, but are not limited to, (1) keeping track of data generation, preparation, and movement steps, (2) having to switch between several environments to do specific tasks (e.g., visual inspection), and (3) saving analysis summary and results in a place that’s as close to the scripts as possible."
  },
  {
    "objectID": "posts/comments-on-data-analysis-workflow/index.html#introduction",
    "href": "posts/comments-on-data-analysis-workflow/index.html#introduction",
    "title": "Comments on data analysis workflow",
    "section": "",
    "text": "There are several benefits of establishing a good and routine data analysis workflow that you follow on a daily basis. At least two benefits come to mind immediately.\n\nHaving a good data analysis workflow is beneficial and needed to do reproducible research/work (RR). RR could mean one thing to one and quite another to others, but to me, doing reproducible work means specifically doing my work in a fashion that allows me to pick up from last touched point in that particular work after some break. For example, after spending 2-3 days on project A, I might go off to do something else for the next 2-3 days, and when I come back to project A, I want to know exactly what’s been done and what I need to do. There are many components of data analysis workflow that can be helpful for one to do RR, and I try to outline them below.\nA routine workflow helps us avoid mental fatigue. This may be from one of the books by Hadley, but to me, this means knowing what, where, why, and how to do things right off the bat when starting a new project. Slowly but surely, I’m settling down to a specific approach that I’ll touch upon later, but to get to this point, I tried several different alternatives, and many times ended up failing, for example, to keep track of how to do things (e.g., refresh data with new source data) all over again when needed. Typical challenges have been, but are not limited to, (1) keeping track of data generation, preparation, and movement steps, (2) having to switch between several environments to do specific tasks (e.g., visual inspection), and (3) saving analysis summary and results in a place that’s as close to the scripts as possible."
  },
  {
    "objectID": "posts/comments-on-data-analysis-workflow/index.html#history",
    "href": "posts/comments-on-data-analysis-workflow/index.html#history",
    "title": "Comments on data analysis workflow",
    "section": "History",
    "text": "History\nNext, I outline what I have tried so far with a quick rundown of pros and cons for each, beginning with the earliest approach I tried at my current work.\n\nLocal RStudio: suitable for most of my personal needs, I’ve been using RStudio on my local computer for a while before joining current work. And even at current work, RStudio was my first tool of choice.\n\nPros\n\nQuick and easy start: fire up an RStudio session (preferably at a Rproj level) and you’re ready for work.\nOne stop shop: everything you need, you can do it in RStudio pretty much, such as visuals, Rmarkdown reports, blogging, version control (git), etc.\n\nCons\n\nPrivacy and security: the biggest drawback of using RStudio at work is that many times we’re not allowed to download the data to local computer due to privacy and security reasons. This is one single biggest roadblock that ultimately kept me from using it as a tool of choice at work.\n\n\nRemote RCloud: an in-house analytics and visualization platform, it allows visual inspections, collaboration among data folks, and many other things, such as shiny app. Once RStudio turned out to be no-go for most tasks at work, I turned to RCloud for everything from data loading and summary, back when it was relatively early (yet still stable) in the development stage.\n\nPros\n\nWork with company data (small and big): since it’s built in-house, it’s capable of consuming company data.\nVisualization: unlike a remote R shell, visuals in RCloud is just like that in RStudio.\nCells with not just R, but shell, python, shiny, etc.: RCloud can be used to do work not just in R, but also in python and shell.\nNotebooks: data generation, preparation, plots, and reports can be all saved in a sensible manner.\n\nCons\n\nConnection sometimes lost (usually for long running jobs): mostly in its early days, I experienced RCloud hanging from time to time especially for long running jobs, typically big data ingestion, complex tasks (modeling), etc.\nNeed internet/browser: not a biggie, but sometimes need to log in too, and it’s not as quick and easy as starting up a local RStudio session, for example.\n\n\nRemote R shell: log on to work server, and start up R session!\n\nPros\n\nQuick and easy start: just like starting up a local RStudio, starting up a remote R shell is quick and easy, once sshed into remote servers.\nCan work with company data (usually small, but sometimes big too): it’s safe to work with company data in the remote servers, and depending on the need, rather big data can be explored in a remote R session too.\n\nCons\n\nCan’t do visual inspections: one biggest hurdle with this option is its lack of graphic support.\nMany times data come from various sources and need to be prepared in a separate environment (e.g., hive)\n\n\nPyspark in remote python shell\n\nPros\n\nScalable solutions for production deliverables\n\nCons\n\nSpark environment in general fails with error messages that are not easy to back track (i.e., I don’t know what I’m doing wrong)\n\n\nSparklyr in remote R shell\n\nPros\n\nScalable solutions in familiar R environment\nFamiliar environment (basically R shell)\n\nCons\n\nUnstable? Connection lost frequently (probably because I’m doing something wrong)\nNo visualization (same problem as in remote R shell)\nSet up time is “long” compared to non spark R shell\n\n\nSparklyr in RCloud\n\nPros\n\nScalable solutions in familiar R environment\nVisualization\n\nCons\n\nBig data visualization is not too common (e.g., many times need aggregation/summarization for plotting anyway, which doesn’t need to happen in spark, but in hive)\nInternet/browser"
  },
  {
    "objectID": "posts/comments-on-data-analysis-workflow/index.html#current",
    "href": "posts/comments-on-data-analysis-workflow/index.html#current",
    "title": "Comments on data analysis workflow",
    "section": "Current",
    "text": "Current\nBefore starting to settle down on a particular workflow, I realized that first thing I needed to do was to identify my day-to-day needs. Not exhaustively, answers to below questions may provide better insights to what we do on a daily basis.\n\nWhat kind of data do I work with the most?\nWhere do the data come from?\nWhat do I do with the data?\nDo I need interactive environment or batch jobs?\nHow about ML? Is production ML an important part of my job?\n\nIt turns out that more often than not, majority of my day-to-day needs are:\n\nExplore data stored in hive tables\nVisualize data stored in hive tables\nSummarize/document analysis results\nTrain ML models and deliver scores in production\n\nSo it became apparent that the integration of R and hive is rather important in my day-to-day work, and that I still rely much on an interactive environment especially during early stages of a project (aka exploration), which takes indeed the majority of the project time.\nHence, I needed to come up with a better way to use data stored in hive tables in an interactive R session. After several trials-and-errors, I came to the conclusion that the combination of bash shell scripts, remote R shell, and RCloud should do for majority of what I do during this stage. This workflow uses bash shell scripts and remote R shell for quick data exploration and additional data prep, typically followed by RCloud for visualization and summary/documentation.\nBelow, I describe how this workflow typically works.\n\nCreate a project directory and start up remote R shell from there\n\nOne screen session and one R shell per project (hence don’t be afraid to have multiple screen sessions running)\n\nQuick/iterative exploration in remote R shell\n\nData generation and preparation\n\nTypically involves (intermediate) hive table generations and loading them in R (hive sql, bash, R shell)\nUse custom bash scripts and internal R packages for common tasks\n\nDescriptive\n\nTypical R operations without visuals\n\nAdditional data prep needed for further actions\n\nTypically for visual inspection in RCloud\n\nIntermediate data saving\n\nTypically in RDS format for visual inspections in RCloud\n\nIterate above steps as needed\n\nVisual inspection and summary in RCloud\n\nCreate a new notebook in RCloud (under a proper higher level directory)\nStart documenting findings from the exploration step\nVisual inspection\n\nUsing RDS files saved from quick explore step\n\nReports and summary\n\nQuick notes on findings according to the visuals\nAlso quick notes on further actions, if needed\n\n\nFurther actions typically involve ML\n\nRemote R shell for POC\nRemote R/python programming\nScalable solution if needed (Spark)\n\n\nThis workflow is not free of pitfalls of course. Some pros and cons are:\n\nPros\n\nIt allows a quick project start-up and exploration.\nData generation, prep, movement, and analysis steps are all stored in once file (explore.R).\nVisual inspection and progress/insights summary are all in one place (RCloud).\n\nCons\n\nIt involves many data writes, which can be costly in terms of time and disk space."
  },
  {
    "objectID": "posts/comments-on-data-analysis-workflow/index.html#links",
    "href": "posts/comments-on-data-analysis-workflow/index.html#links",
    "title": "Comments on data analysis workflow",
    "section": "Links",
    "text": "Links\n\nhttps://www.tidyverse.org/articles/2017/12/workflow-vs-script/\nhttp://r4ds.had.co.nz/explore-intro.html\nhttps://edwinth.github.io/blog/workflow/\nhttps://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext"
  },
  {
    "objectID": "posts/vim-and-slime/index.html",
    "href": "posts/vim-and-slime/index.html",
    "title": "Vim, vim-slime, and screen",
    "section": "",
    "text": "As much as I love using RStudio for everything1, there are times when I can’t really use it, i.e., for some projects at work in which I need to log in to some servers and do work there (i.e., on the server side). Using RStudio server would do it, I think, but it’s not an option at the moment. For a long time, I’ve been trying to establish a good working data analysis workflow that I can follow in those situations.\nVim is my server side editor of choice. I’m not a Vim expert by no means, but ever since I started using it (it’s been almost 4 years), it fit me, and it’s been my editor of choice, especially when working on some remote servers2. I would open up several terminal window tabs, i.e. one for command line, one for R session, one for file editing (using Vim), one for hive shell, etc..Establishing an effective data analysis workflow came down to being able to use Vim more seamlessly with those other tools that I use everyday.\nAfter several days of browsing, reading, and trying out, I finally settled down with this particular set-up and so far I’m pretty happy with it. This post is a quick overview of the set-up as a reference for myself and potentially others who’s facing a similar situation. This is one of those things that I wish I had perfected while I was still at school, just like many other things that I’ve written about before here.\nI searched web for information regarding Vim set up for data analysis, especially using R. Very quickly, I ran into Nvim-R, a Vim plugin to edit R code. I tried it for a couple of days, but ended up dropping it, because as much as I love R, I had to use some other tools with Vim, and I needed more general solution, not an R-specific solution.\nBut that turned out to be my introduction to the world of Vim plugins! I spent a couple of nights searching for cool Vim plugins, sometimes for the ones that are not necessarily related to doing data analysis. But it was so fun trying out a variety of Vim plugins. This is also when I started using Vim plugin manager as well, which makes installing/uninstalling Vim plugins simple and easy. I tried only one such manager, Vundle, and have been using it ever since.\nNow to use a Vim plugin manager and several plugins, I started adding more stuffs in my .vimrc file, which is like a .Rprofile file (or .bashrc). It contains many settings for Vim, i.e., plugins related specific setting as well as more general settings. And boy are there tons of cool stuffs you can do with these settings! Of the many general settings, one of my favorite is insert mode mapping of “jj” (two strokes of the key j) to esc key, so that I don’t have to reach for the esc key while typing, and instead just type jj to get back to normal mode3. Like I said, I spent several couple of nights trying out different settings and plugins, and I had to resist the urge to play with new settings and plugins. It’s like trying to perfect how your blog looks like, and I had to try hard not to spend too much time perfecting my .vimrc file, i.e., my Vim setup.\nThen I ran into Vim-slime, another Vim plugin that allows you to send lines of code from one window to another, thus eliminating the need for constant copying from a text file (e.g., data analysis script file) and pasting on R session, for example. It’s the equivalence of selection and cmd-enter in RStudio to run select lines of code.\nWith Vim-slime, I can send select lines of codes from one terminal window tab to another, be it an R console, Python console, Hive shell, etc. So it provided me with a more general solution than Nvim-R, which is still a great R specific plugin.\nLast but not least, I had to come up with a way to keep my running terminal sessions even after server connection is lost. I have several meetings a day, and sometimes I lose my server connection while walking to/from a meeting. I already knew and have been using screen for that purpose, and it so happens that Vim-slime works great with screen. By default, a chunk of code I want to run is sent to a screen session, and I learned how to name a screen session too.\nI also ran into tmux, which is a screen multiplexer, similar to GNU screen. Essentially just like screen, it lets you work on multiple tabs within one terminal window without losing them even after your server connection is lost. It looked very interesting and useful, but unfortunately, it needed to be installed on the server side, and to make things worse, the server was missing one of the required dependencies, so I decided to drop tmux from my target list.\nSo putting these together, here’s what I typically do first thing in the morning at work. There’s probably a better way to do this still, but until I figure out how to, I’m happy with the current set up.\nSource screenshot\n$screen -S R\nThis will open a screen sesion whose name is “R”. Once in it, start an R shell, (or python, hive, etc.), and let’s call it a “console” tab. I find it helpful to name each of these console tabs with corresponding tool name, e.g., “hive” when using hive shell there, “python” when using python shell there, and “R” when using R shell there, etc..\nVim-slime workflow screenshots\nWhen code chunks are sent for the first time in a session, Vim-slime asks for the destination screen session name, and I entered R, which was my screen session name from step 2 above. It then asks for screen window name, and I just leave it as is (default 0)."
  },
  {
    "objectID": "posts/vim-and-slime/index.html#footnotes",
    "href": "posts/vim-and-slime/index.html#footnotes",
    "title": "Vim, vim-slime, and screen",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEspecially for blogging, yeah!↩︎\nIn fact, I’m using Vim mode in RStudio too.↩︎\nI wonder if there’s a way to map jj to esc in RStudio vim mode too.↩︎"
  },
  {
    "objectID": "posts/r-package-development-presentation/index.html",
    "href": "posts/r-package-development-presentation/index.html",
    "title": "External presentation goal",
    "section": "",
    "text": "Like Yihui (of blogdown and many other awesome R packages) whose goal is to publish a book a year1, I have a similar personal goal that I started last year. While I’d love to write a book a year, it’s too ambitious a goal for me (and many people in general, I’d think) to pursue. Instead, my personal goal is to do an external presentation a year, be it for meetups, conferences, or nearby schools as a guest speaker.\nBack in May, I did a presentation on R package development for the ATL R User group, successfully completing the mission for the year :) The presentation has two decks: (1) main deck (here) containing the introduction and high level overview, and (2) reference deck2 (here) containing the screenshots of each step of the R package development that I covered during the presentation.\nI should probably add to my personal goal list something about blog posting3. I’m just happy still that blogging is this easy (again thanks to Yihui!), and so there should be no excuse why I can’t write a post more frequently. Well, I won’t make it an official personal goal just yet, but for now, I will have to keep writing posts whenever I can, sometimes multiple posts at one sitting, just like today! :)"
  },
  {
    "objectID": "posts/r-package-development-presentation/index.html#footnotes",
    "href": "posts/r-package-development-presentation/index.html#footnotes",
    "title": "External presentation goal",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAmazingly, he’s successfully written a book a year for the last couple of years as far as I know.↩︎\nwhich I talked about here↩︎\nYihui’s frequent blog posting is also amazing.↩︎"
  },
  {
    "objectID": "posts/package-update/index.html",
    "href": "posts/package-update/index.html",
    "title": "Package uncmbb updated!",
    "section": "",
    "text": "Ever since UNCMBB team’s season ended about a month ago, I’ve been meaning to update the data in uncmbb package, and I finally got to it. Obviously the new version includes 2017-2018 season match results for both UNC and Duke MBB teams, but it also includes new features in wrapper functions that are useful when working with uncmbb package.\nI forgot to take notes of the steps taken when I was submitting uncmbb to CRAN for the first time last year, and this time I remembered to do so and take some screenshots too. Below are some notes related to package submission:\nsuppressPackageStartupMessages(library(uncmbb))\nsuppressPackageStartupMessages(library(dplyr))\n\ntail(unc)\n\n     Season  Game_Date Game_Day   Type Where      Opponent_School Result Tm Opp\n2256   2020 2020-02-25      Tue    REG     H North Carolina State      W 85  79\n2257   2020 2020-02-29      Sat    REG     A             Syracuse      W 92  79\n2258   2020 2020-03-03      Tue    REG     H          Wake Forest      W 93  83\n2259   2020 2020-03-07      Sat    REG     A                 Duke      L 76  89\n2260   2020 2020-03-10      Tue CTOURN     N        Virginia Tech      W 78  56\n2261   2020 2020-03-11      Wed CTOURN     N             Syracuse      L 53  81\n       OT\n2256 &lt;NA&gt;\n2257 &lt;NA&gt;\n2258 &lt;NA&gt;\n2259 &lt;NA&gt;\n2260 &lt;NA&gt;\n2261 &lt;NA&gt;\n\nduke %&gt;% mbb_season_result() %&gt;%\n        filter(Type == \"REG\") %&gt;%\n        arrange(desc(pct_win)) %&gt;%\n        head(5) \n\n# A tibble: 5 × 7\n# Groups:   Season [5]\n  Season Type  games  wins losses pct_win pct_loss\n  &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 1999   REG      30    29      1   0.967   0.0333\n2 1986   REG      31    29      2   0.935   0.0645\n3 1998   REG      29    27      2   0.931   0.0690\n4 1992   REG      27    25      2   0.926   0.0741\n5 1963   REG      23    21      2   0.913   0.0870\n\nunc %&gt;% mbb_champ_season()\n\n# A tibble: 6 × 1\n# Groups:   Season [6]\n  Season\n  &lt;chr&gt; \n1 1957  \n2 1982  \n3 1993  \n4 2005  \n5 2009  \n6 2017"
  },
  {
    "objectID": "posts/package-update/index.html#footnotes",
    "href": "posts/package-update/index.html#footnotes",
    "title": "Package uncmbb updated!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI say it almost here, because there’s literally last step one has to do before submitting to CRAN.↩︎\nYes it’s quite cumbersome, and there’s got to be a better way to do this that I don’t know of yet.↩︎\nhttps://juliasilge.com/blog/beginners-guide-to-travis/↩︎\nThe badges ended up giving me some headache during CRAN submission.↩︎\nI didn’t do this after receiving the confirmation email first time (after devtools::release() was completed), and kept wondering why there was no more update coming from CRAN.↩︎"
  },
  {
    "objectID": "posts/one-year-after-rstudio-conf-2017/index.html",
    "href": "posts/one-year-after-rstudio-conf-2017/index.html",
    "title": "One year after rstudio::conf 2017",
    "section": "",
    "text": "It was merely a name change, but with the new blog address at rbind.io, I’m excited about the fact that I am a blogger who uses his name in his blog address, because I’ve always considered it to be one of the coolest things you can do online :) And it got me thinking, where did it all start from? After many failed attempts to start and maintain my own blog, how is it that this time, it seems it’ll just work?\nA quick answer was @yihui and blogdown package, along with amazing R community/support, but besides blogging, I knew I did one or two other cool things last year, and quickly I realized that it all started with the rstudio conference I attended in 2017. It’s probably not a coincidence that I came to this realization right before the week of rstudio conference 2018, because my twitter feed is all mentions related to the conference. With that I got to think about other cool things I ended up doing last year, particularly since attending the rstudio conference.\nSo here is a quick list of other cool things I’ve done with a brief comments under each, while at and since attending the rstudio conference in January, 2017. I tried to keep them in a chronological order.\n\nHadley’s autograph\n\nI took Hadley’s Master R Developer workshop, since having worked with R for a couple of years now, I wanted to write an R package at some point. It was simply a great workshop, and yes I asked for his autograph on the first page of his Advanced R book, which he did :)\n\nStarted using github\n\nI never used github before. I created an account on github some time ago, but never actually used it. After taking the training and listening to the talks at the conference, I decided not only to start using it, but to use it frequently. Version control was never a part of my education/work, and I think it’s great seeing many folks having pushed/pusing for the inclusion of this topic in data science/statistics curriculum these days. This is just one of those things that I wish I had learned while at school.\n\nWrote my first R (data) package, now available on CRAN\n\nOne of the coolest things I’ve always considered as an avid R user is having your own R package available on CRAN. Hadley’s Master R Developer workshop was the single biggest reason I wanted to attend the conference in 2017, and not so long after the conference, I had my first R package up on CRAN. I’m planning to write about starting your data analysis journey with creating an R data packge of your own.\n\nStarted preaching R package, tidyverse, and reproducible research (RR) at work\n\nI’ve been the R guy at work even before the rstudio conference, but after attending the Hadley’s training and embracing the idea of developing R packages for projects especially for the purpose of doing reproducible research (casually, I call it reproducible work instead) over other work artifacts and habits, I’m that R guy even more who’s introducing tidyverse approach and sharing tips for doing reproducible work. Again, I’m hoping to write about data analysis workflow that I came to settle on that helps with doing RR at work.\n\nStarted having a second thought about R packages and tinkered with .Rprofile\n\nAs I tried to incorporate R packages for projects at work, soon I ran into several roadblocks mainly due to inherent limitations of corporate world. I’m planning to write about this some time later too, but the main issue was that there are times that I can’t use RStudio IDE at work and instead have to work with other tools, mainly in R command line in company servers (which I’m happy with, but not at the same level as using RStudio IDE). Since I wasn’t making a good progress in incorporating packages for projects, I started using .Rprofile files to put frequently used functions in one place, but surely I ended up running into the challenges of easily sharing my work with team members.\n\nTurned around once again and made a second R package, primarily for use at work (local).\n\nNot long after I shared my frustration with the difficulty of R package development under corporate environment and my use of .Rprofile file with my team members, I found a way to solve the particular problems I was having. I ended up ditching the use of .Rprofile file completely, and now when I run into such challenges (that are inherent to large company environments), I try to find a solution within existing limitations somehow. Definitely, many times working in a corporate environment can be inefficient and slow, but it’s still no reason to use .Rprofile for frequently used functions.\n\nStarted a blog, now using rbind.io subdomain.\n\nAt the conference, I didn’t really pay attention to @yihui’s talks on blogdown. As my first R package became available on CRAN, however, I wanted to share a couple of quick data analyses that can be done with the data, and quickly I ran into a couple of how-to’s using blogdown. There’s been many failed attempts to create and maintain my own blog in the past, but I was able to start blogging and keep doing it still, thanks to @yihui and other great folks in R community.\nUnfortunately, I’m not going to the rstudio conference this year (training/conference schedule just didn’t work out), and boy do I want to be there this week. For those of you lucky attendees, I hope you make the best out of the conference and enjoy everything you get to learn from amazing people and do your own cool/amazing things after the conference.\nI know you will :)"
  },
  {
    "objectID": "posts/points-difference-season-average/index.html",
    "href": "posts/points-difference-season-average/index.html",
    "title": "Average points difference through first N games",
    "section": "",
    "text": "Time flies, and the Tar Heels played the season’s first 5 games already. And loosely speaking, they’ve played well so far (knocking on woods), to the eyes of just another casual fan here!\nI’m not going to say anything about this season’s fate based on the results of the season’s first 5 games, but still I was curious how well this season compares to other seasons at this point. There could be many different ways to do just that, but I thought looking at each season’s running average of points difference after each regular season game might be interesting.\nLooking through the 5th game of each season, the current season sits at 19th out of total 69 seasons since 1949-50 campaign as it shows in the next table. Also note that 3 of the top 6 seasons in the table (2017, 1993, and 2009) were championship winning seasons.\n\n\n\nAverage points difference through season's first 5 games (showing 5th game opponent)\n\n\nRank\nSeason\nOpponent\nTm\nOpp\nAvg_Diff\n\n\n\n\n1\n2019\nSaint Francis\n101\n76\n30.6\n\n\n2\n2017\nChaminade\n104\n61\n28.8\n\n\n3\n1998\nSeton Hall\n95\n65\n28.4\n\n\n4\n1973\nKentucky\n78\n70\n27.6\n\n\n5\n1993\nHouston\n84\n76\n27.6\n\n\n6\n1987\nMiami\n122\n77\n26.8\n\n\n7\n2009\nOregon\n98\n69\n25.0\n\n\n8\n1992\nSeton Hall\n83\n54\n24.8\n\n\n9\n1970\nVirginia\n80\n76\n24.2\n\n\n10\n1974\nEast Tennessee State\n81\n63\n23.4\n\n\n11\n1986\nNevada-Las Vegas\n65\n60\n23.4\n\n\n12\n1991\nConnecticut\n79\n64\n23.4\n\n\n13\n1976\nEast Tennessee State\n104\n67\n23.0\n\n\n14\n1972\nWake Forest\n99\n76\n22.6\n\n\n15\n2008\nBrigham Young\n73\n63\n22.4\n\n\n16\n2012\nSouth Carolina\n87\n62\n22.4\n\n\n17\n1967\nNew York University\n95\n58\n22.0\n\n\n18\n1969\nClemson\n90\n69\n19.4\n\n\n19\n2004\nGeorge Mason\n115\n81\n19.2\n\n\n20\n2018\nArkansas\n87\n68\n19.2\n\n\n\n\n\n\n\nWhen we look at the running average of points difference each season, it seems to be stablizing after first couple of games each season. Maybe it’s after the first 5 games, or maybe it’s after the beginning of the conference games, but regardless, considering first few games each season are usually “easier” than later games, that observation seems to make sense.\nI tried to distinguish three different types of seasons in the plot: (1) 6 NCAA championship winning seasons (in black), (2) current season (in carolin blue), and (3) all the other seasons (in grey).\n\n\n\n\n\n\n\n\n\nAnd what’s with that high flying season in grey that had average points difference of 30+ through the first 15 games ?\n\n\n\nAverage points difference through season's first 15 games (showing 15th game opponent)\n\n\nRank\nSeason\nOpponent\nTm\nOpp\nAvg_Diff\n\n\n\n\n1\n1986\nFordham\n92\n68\n33.93333\n\n\n2\n2009\nCollege of Charleston\n108\n70\n25.00000\n\n\n3\n2005\nGeorgia Tech\n91\n69\n24.93333\n\n\n4\n2007\nFlorida State\n84\n58\n24.80000\n\n\n5\n1993\nClemson\n82\n72\n24.26667\n\n\n\n\n\n\n\nIt turns out it was the 1985-86 season, and it must have been a somewhat disappointing season without the championship trophy after blazing through opponents early in the season. Seems like their last game was against U of L.\n\n\n\n\n\nSeason\nGame_Date\nType\nWhere\nOpponent_School\nResult\nTm\nOpp\n\n\n\n\n1986\n1985-11-24\nREG\nH\nUCLA\nW\n107\n70\n\n\n1986\n1985-11-25\nREG\nH\nIona\nW\n110\n67\n\n\n1986\n1985-11-29\nREG\nN\nMissouri\nW\n84\n63\n\n\n1986\n1985-11-30\nREG\nN\nPurdue\nW\n73\n62\n\n\n1986\n1985-12-01\nREG\nN\nNevada-Las Vegas\nW\n65\n60\n\n\n\n\n\n\n\n\n\n\n\nSeason\nGame_Date\nType\nWhere\nOpponent_School\nResult\nTm\nOpp\n\n\n\n\n30\n1986\n1986-03-02\nREG\nA\nDuke\nL\n74\n82\n\n\n31\n1986\n1986-03-07\nCTOURN\nH\nMaryland\nL\n75\n85\n\n\n32\n1986\n1986-03-13\nNCAA\nN\nUtah\nW\n84\n72\n\n\n33\n1986\n1986-03-15\nNCAA\nN\nAlabama-Birmingham\nW\n77\n59\n\n\n34\n1986\n1986-03-20\nNCAA\nN\nLouisville\nL\n79\n94\n\n\n\n\n\n\n\nNext up Spartans! Go Heels!"
  },
  {
    "objectID": "posts/blog-at-rbind-io/index.html",
    "href": "posts/blog-at-rbind-io/index.html",
    "title": "Blog at rbind.io",
    "section": "",
    "text": "This weekend, I made a small but meaningful change to the blog that I started late last year. Initially, my blog was named UNCMBBTRIVIA to emphasize to the readers that this blog is about my beloved UNC Men’s Basketball team’s trivia. But I always wanted to have a blog where I write not only about UNC MBB trivia, but more importantly about data analysis and R.\nAs I thought about it more, it became clear that I’d better drop the UNCMBBTrivia-only identity of the blog to start writing more about data analysis and R, and ever since then I’ve been thinking about how and when to make the change. Searching for ideas, this Friday I was reading this and that, and bam! I ran into Yihui’s online blogdown book, in which he introduced *.rbind.io subdomain option for deployment (via netlify, which was my original choice).\nEverything about *.rbind.io option was just right for what I wanted: moving away from UNCMBBTrivia-only identity, showing a taste of data-powered contents of the blog, and being more active in the R community (who among useRs are not excited about interacting with @yihui, @hadley, @drob, and many more amazing folks in R community?)\nSo this weekend, I jumpted the gun and created an issue on rbind support github, which is what one needs to do to request for an rbind.io subdomain. To my pleasant surprise, @road2stat responded to my request quickly, and less than 24 hours later, the new blog address is joongsup.rbind.io :) (I wonder if there’s a way to automatically redirect the visit to the original netlify address to the new rbind.io one? And there’s a way to do it. See here.)\nI couldn’t be happier with the whole experience, and am looking forward to sharing my journey in data analysis and R in addition to UNCMBBTrivia here."
  },
  {
    "objectID": "posts/5th-year-players/index.html",
    "href": "posts/5th-year-players/index.html",
    "title": "5th year players",
    "section": "",
    "text": "As I mentioned it in an earlier post, I’ve been wanting to add each season’s players info to my data source. Although it needs more cleaning still, I finally got around to it and got to play with the players data a bit lately.\nAs part of cleaning the players data, I needed to make sure that for most players, their college careers lasted at most for four years. At one end of the extreme sit those one-and-done players (e.g., Tony Bradley, Marvin Williams); at the other end are the 5th year players.\nFresh in memory from reading the 2009 championship book (“One Fantastic Ride”) by Adam Lucas, I knew about Marcus Ginyard’s case. He hurted one of his legs early in his senior season and sat out the remainder of what turned out to be the championship season. Also I remember reading an article about Stilman White taking a mission trip while in college. I was curious if there were other players like them.\n\n\n\n\n\nPlayer\nfirst_yr\nlast_yr\nnote\n\n\n\n\nTony Radovich\n1953\n1957\nSomehow played for 5 seasons.\n\n\nGeoff Crompton\n1974\n1978\nRedshirted 2nd year.\n\n\nWarren Martin\n1982\n1986\nRedshirted 3rd year.\n\n\nCurtis Hunter\n1983\n1987\nRedshirted 2nd year.\n\n\nKevin Madden\n1986\n1990\nRedshirted 2nd year.\n\n\nPat Sullivan\n1991\n1995\nRedshirted 4th year.\n\n\nBrian Bersticker\n1998\n2002\nRedshirted 3rd year due to an injury.\n\n\nOrlando Melendez\n1998\n2002\nRedshirted 1st year due to a foot injury.\n\n\nMarcus Ginyard\n2006\n2010\nRedshirted 4th year due to a foot injury.\n\n\nLeslie McDonald\n2010\n2014\nRedshirted 3rd year due to a knee injury.\n\n\nStilman White\n2012\n2017\nAfter 1st year, spent 2 years in mission trip.\n\n\n\n\n\n\n\nOther than the three most recent players on the list (e.g., White, McDonald, and Ginyard), I had no idea who the remaining eight players were, thus needing googling and cross checking various sources I could find online. It was not a very comprehensive searching effort, but not surprisingly, this quick researching revealed that many of them took a year off due to injuries of various kinds, although it was hard to find exactly what kinds of injuries led players to redshirt a year expecially for those who played in pre 90s.\nBest of luck to the players in the 2017-18 roster who open the season this Friday, and let’s hope for an injury-free and awesome season (knocking on wood)!"
  },
  {
    "objectID": "posts/same-day-match-results-unc-and-duke/index.html",
    "href": "posts/same-day-match-results-unc-and-duke/index.html",
    "title": "Same day match results: UNC and Duke",
    "section": "",
    "text": "So both UNC and Duke lost today (Saturday, 2018-01-06). I’d think it’s pretty rare that such event (both teams losing on a same day) happens, and I got curious how rare it is indeed. What are the odds of both teams winning, losing (like today), and splitting?\nI knew both teams have approximately 75% win percentage (since 1949-50 season), so I’m guessing on any given day, the probability of both teams winning should be close to 0.75 x 0.75 = 0.5625, assuming either team’s game result doesn’t really impact the other team’s result. Similarly, both teams losing at the same time should be close to 0.25 x 0.25 = 0.0625. Let’s see if history tells us otherwise.\n\n\n`summarise()` has grouped output by 'unc_result'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nIt seems like the quick calculations seem to match the actual game results. Between 1949-50 and 2016-17 season, there were total 835 days in which both UNC and Duke played, not with each other, and it’s indeed rare that both teams lose on a same day (48 out of 835 days).\nLooking at the split days (W/L and L/W days), it seems UNC gathered more Ws than Duke did when they played on a same day. Let’s see how they played when they played on a same day (again not with each other, but playing some other teams).\n\n\n`summarise()` has grouped output by 'Type'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'Type'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nSo indeed, UNC have won slightly more matches than Duke did on days in which they both played, in all match types.\nOne thing seems odd, though. Why have UNC played 719 regular matches when Duke played one fewer games in 718 if we’re talking about games played on same days? And reciprocally for CTOURN, Duke (76) played one more game than UNC (75). They should have played the same number of games, like that for NIT and NCAA.\n\n\n\nOne extra regular game for UNC\n\n\nSeason\nGame_Date\nGame_Day\nType\nWhere\nOpponent_School\nResult\nTm\nOpp\nOT\nteam\n\n\n\n\n1955\n1955-03-03\nThu\nCTOURN\nH\nWake Forest\nL\n82\n95\nNA\nUNC's Record\n\n\n1955\n1955-03-03\nThu\nCTOURN\nH\nSouth Carolina\nW\n83\n67\nNA\nDuke's Record\n\n\n\n\n\n\n\nA quick investigation and search reveals that the UNC vs. Wake Forest game on 1955-03-03 was an ACC conference tournament game (CTOURN), not a regular season game. So it looks like the data source I’m using seems to have an incorrect record there. Although nothing big, another good gotcha while exploring the match data :)"
  },
  {
    "objectID": "posts/my-old-coding-products/index.html",
    "href": "posts/my-old-coding-products/index.html",
    "title": "My old coding products",
    "section": "",
    "text": "Lately I’ve been thinking about why I care much about everything R and sharing the joy of using R, which deserves its own post. Much of it has to do with how I did and did not get a proper training in coding suitable for data analysis in the past, but as I looked back on my personal coding history, I came across hundreds of code files that I wrote in the past1. Suffice to say, they were pretty important files in my education history, but I have totally forgotten about them until now.\nThey are .m files, meant to be used in Matlab. As I started opening and reading a couple of those files, I had mixed feelings, ranging from agony (“omg, this code was from that stressful point in time”) and embarrassment (“omg, look at the logic and style I was using there”), to delight (“wow I’m defining several functions to be used in main script!”) and motivation (“Yup, I was this bad, and that’s exactly why I care about education in coding (especially R) for data analysis!”).\nIt was quite refreshing to see how I used to code in Matlab, which had some similarities with R, hence giving me some perspectives on how my data analysis coding skills might have changed. Here are some observations of the past, in conjunction with present whenever applicable:\nIt was quite a pleasant surprise running into my past code prouducts. I don’t think I’ll ever code in Matlab again, but it was still quite refreshing to learn how I used to do it, especially from the current point of view. I wonder how I’d feel about my current coding in say next 5-10 years!?"
  },
  {
    "objectID": "posts/my-old-coding-products/index.html#footnotes",
    "href": "posts/my-old-coding-products/index.html#footnotes",
    "title": "My old coding products",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFortunately, I’ve kept those files in a portable device, which I’m not usually good at doing.↩︎\nInterestingly, it seems I didn’t have to “source” the files to use the functions. Matlab must have known where to look for the function names.↩︎\nE.g., see here.↩︎"
  }
]